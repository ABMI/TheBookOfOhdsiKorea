# Clinical Validity {#ClinicalValidity}

*Chapter leads: Joel Swerdel, Seng Chan You, Ray Chen & Patrick Ryan*

> 질량을 에너지로 바꾸는 확률은 마치 몇 마리의 새만 존재하는 나라에서 눈을 감고 총을 쏘아 나는 새를 맞출 확률과 비슷하다.  *알베르트 아인슈타인, 1935*

OHDSI의 이상은 '관찰형 연구를 통하여 세계에 건강과 질병에 대한 포괄적인 이해를 제공' 하는 것이다. 후향적 연구는 기존재하는 데이터를 이용할 수 있다는 편리함이 있지만, 이전 챕터\@ref(EvidenceQuality)에서 기술한 바와 같이 다양한 타당성의 한계를 지니고 있다. 데이터의 질과 통계적 방법론과 별개로 임상적 타당성 (clinical validity)를 논하기는 쉽지 않지만 이번 챕터에서는 보건의료 데이터의 특성, 코호트 정의 검증, 근거의 일반화, 이 세 가지에 집중해보려 한다. poulation-level estimation 챕터\@ref(PopulationLevelEstimation)의 예제로 돌아가보자. 우리는 'ACE inhibitor가 thiazide 또는 thiazide-like diuretics 에 비해 혈관부종 위험성이 높은가?'에 대해 대답하려 했었고, 우리는 ACE inhibitor 가 thiazide 또는 thiazide-like diuretics 에 비해 혈관부종을 더 많이 야기함을 증명했었다. 이번 챕터는 다음의 질문에 대해 답하기 위해 쓰여졌다. "수행한 분석이 어느 정도로 임상적 의도와 일치하는가?" \index{clinical validity}

## 보건의료 데이터의 특성 (Characteristics of Health Care Databases) {#CharacteristicsOfDatabase}

우리가 확인했던 것이 사실 ACE inhibitor **사용** 과 혈관부종의 관계가 아니라, ACE inhibitor의 **처방** 과 혈관부종 간의 관계일 수도 있다. 우리는 이전 챕터(\@ref(DataQuality))에서 데이터의 질에 대해서 이미 다루었다. 공통데이터모델 (common data model, CDM)으로 변환된 데이터의 질은 원본 데이터의 질을 결코 넘어설 수 없다. 여기서 우리는 대부분의 의료 서비스 사용 데이터의 특성에 대해 다룬다. OHDSI의 많은 데이터베이스는 청구 데이터 또는 전자의무기록 (electronic health record, EHR)에서 유래된다. 청구데이터와 전자의무기록 모두 연구를 위해서 만들어진 데이터베이스가 아니며, 둘은 서로 다른 데이터 수집 과정을 거친다. 청구 데이터는 보험금 청구 및 환급을 위하여 만들어진 데이터베이스로, 데이터 요소들은 제공된 의료 서비스의 청구를 정당화하기 위한 목적 하에 만들어진다. 전자의무기록의 데이터 요소들은 임상 의료 행위 및 운영을 뒷받침하기 위하여 수집되며, 주어진 의료 시스템 기반에서 현재의 의료 서비스 및 추후 필요하리라 예상되는 정보 위주로 수집된다. 이들 모두 환자의 완전한 병력 (medical history) 를 반영하거나 서로 다른 의료 기관 간의 데이터를 통합하지 못한다.

관찰형 데이터로부터 믿을만한 근거를 생성하기 위해서는, 연구자들은 환자가 의료 서비스를 찾는 그 순간부터 의료 서비스에 대한 데이터가 만들어져 분석에 사용되기 까지의 전 과정에 대해 이해할 필요가 있다. 예를 들어 다양한 원천 관찰형 데이터에서 "약물 노출 (drug exposure)"은 의사의 처방 기록, 약국 조제 기록, 병원에서의 직접 주입, 또는 환자가 자가보고한 약물 복용 기록까지 여러 가지를 의미할 수 있다. 데이터가 어디에서 유래했는지 (data source)는 환자가 약을 실제로 복용했는지 안 했는지 뿐 아니라 약물 복용 기간에 대한 신빙성에 영향을 미칠 수 있다.  데이터 수집 과정은 의사 처방 없이 복용할 수 있는 over-the-count (OTC) 등의 약물 노출을 과소 평가할 수 있고, 환자가 처방만 받고 실제로 약국에서 조제 받지 않거나, 복용하지 않은 경우 약물 노출을 과대 평가할 수도 있다. 치료 노출과 outcome 간의 가능한 편향을 이해하고, 보다 이상적으로는 이러한 오류를 정량화하고 보정하는 것은 가용한 데이터로부터 우리가 만들어 낸 근거의 신뢰성을 향상시킬 수 있다.  

## Cohort Validation {#CohortValidation}

@hripcsak_2017 은 "phenotype 은 유기체의 유전적 구성에서 파생된 genotype과 구별되는, 관찰 가능하고 잠재적으로 변화하는 유기체의 상태를 나타내는 것"이라고 설명했다. phenotype 이라는 용어는 전자 의무 기록 (EHR) 데이터로부터 추정되는 환자 특성에 적용될 수있다. 연구자들은 정보학이 시작된 이후 구조화 된 데이터와 서술적 데이터 모두에서 EHR phenotyping을 수행해 왔다. 목표는 원 EHR 데이터, 청구 데이터 또는 기타 임상 관련 데이터를 기반으로 대상 개념에 대한 결론을 도출하는 것이다. Phenotype 알고리즘 (예를 들어, phenotype을 식별하거나 특성화하는 알고리즘)은 공학적인 최근의 연구 또는 다양한 형태의 머신 러닝을 통해 분야의 전문가나 분야 지식을 가지고 있는 엔지니어에 의해 생성될 수 있다.

이 설명은 clinical validity 고려 시 보강에 유용한 몇 가지 속성을 강조한다. 1) 그것은 우리가 관찰할 수 있는 어떤 것에 대해 말하고 있다는 것을 분명히 한다. (따라서 우리의 관찰형 데이터에서 수집이 가능함을 의미한다) 2) 그것은 phenotype 정의에 시간의 개념을 포함한다. (사람의 상태가 변할 수 있기 때문에) 3) 원하는 의도인 phenotype과 의도에 대한 구현인 phenotype 알고리즘을 구별한다.

OHDSI는 "코호트"라는 용어를 채택하여 일정 기간 동안 하나 이상의 포함 기준을 충족하는 사람 집합을 정의했다. "코호트 정의"는 관찰형 데이터베이스에 대해 코호트를 구현하는데 필요한 논리를 나타낸다. 이와 관련하여 코호트 정의(또는 phenotype 알고리즘)는 관찰 가능한 관심 임상 상태에 속하는 환자에 대한 phenotype을 반영하기 위한 코호트를 생성하는 데 사용된다.

Clinical characterization, population-level estimation, patient-level prediction을 포함한 대부분의 관찰형 분석의 연구 프로세스의 일부로 하나 이상의 코호트를 설정해야 한다. 이러한 분석에 의해 도출된 근거의 타당성을 평가하기 위해, 우리는 반드시 각각의 코호트에 대해 다음의 질문을 던져야 한다: '코호트 정의 따라 가용한 관찰형 데이터에서 식별된 코호트의 피험자들이 의도했던 phenotype에 실제로 얼마나 부합하는가?'

다시 \@ref(PopulationLevelEstimation) 장의 예제로 돌아가보자. 'ACE 억제제가 thiazide 계열 이뇨제와 비교해서 혈관부종 위험성을 높이는가?'에 대한 질문에 대답하기 위하여 우리는 3가지 코호트를 생성했다. ACE 억제제를 처음 사용한 대상 코호트, Thiazide 계열 이뇨제를 처음 사용한 대조 코호트, 혈관부종이 발생한 outcome 코호트. 우리는 ACE 억제제와 thiazide 이뇨제를 사용한 사람들을 모두 식별하였다고 얼마나 자신 있게 말할 수 있는가? 처음 사용한 환자 (new user) 라는 조건에 이전 (관찰되지 않은) 사용이 모두 배제되었다고 믿을 수 있는가? 약물에 노출되었다는 기록이 있는 환자가 실제로 약물에 노출되었고, 기록이 없는 환자들은 실제로 약물에 노출되지 않았다고 믿을 수 있는가? ACE 억제제를 복용한다고 했을 때, 코호트 시작 시점과 종료 시점이 실제 약물을 시작하고, 종료한 시점과 일치할까? "혈관부종" 발생의 기록이 있는 환자들이 다른 알레르기 피부 질환과 다른, 실제 피하 조직의 부종을 경험했을 것인가? 실제 혈관 부종을 겪은 환자 중 얼마나 많은 숫자의 환자들이 실제로 의사에게 진찰을 받고 해당 질환을 진단 받고 기록되었을까? 우리가 약물에 의한 것이라고 의심한 혈관부종이 음식 알레르기나 바이러스 감염 등에 의한 다른 원인과 얼마나 구별지어질 수 있는가? 질병의 발생 시기가 약물 노출과 부작용 발생 간의 시계열적 연관성을 도출하기에 확신을 가질 수 있을만큼 잘 포착되었는가? 이러한 유형의 질문에 답하는 것이 임상적 타당성의 핵심이다.

In this chapter, we will discuss the methods for validating cohort definitions. We first describe the metrics used to measure the validity of a cohort definition. Next, we describe two methods to estimate these metrics: 1) clinical adjudication through source record verification, and 2) PheValuator, a semi-automated method using diagnostic predictive modeling.

이 장에서는 코호트 정의를 검증하는 방법에 대해 논의한다. 먼저 코호트 정의의 타당성을 측정하는 데 사용되는 측정기준을 설명한다. 다음으로, 이러한 측정기준을 추정하는 두 가지 방법을 설명한다. 1) 원천 기록 검증을 통한 임상 판정과 2) PheValuator, 진단적 에측 모델링을 이용하는 반자동화 방법이다.

### 코호트 판단 측정기준

연구에 대한 코호트 정의가 결정되면 정의의 타당성을 평가할 수 있다. 타당성을 평가하는 일반적인 접근방식은 정의된 코호트의 일부 또는 모든 사람을 '골드 스탠다드 (gold standard)'에 비교하고 그 결과를 코호트 정의 내의 황금 표준 분류 및 자격 검정에 따라 계층화하여 2x2의 혼동 행렬 (confusion matrix)으로 표현하는 것이다. 그림 \@ref(fig:matrix) 은 혼동 행렬의 요소들을 보여준다.

```{r matrix, fig.cap='Confusion matrix.', echo=FALSE, out.width='75%', fig.align='center'}
knitr::include_graphics("images/ClinicalValidity/matrix.png")
```

코호트 정의의 참과 거짓 결과는 그 정의를 사람들의 집합에 적용함으로써 결정된다. 정의에 포함된 사람들은 특정 건강 상태에 대해 양성으로 간주되며 "양성(true)"으로 표시된다. 코호트 정의에 포함되지 않은 사람들은 건강 상태에 대해 음성으로 간주되며 "음성(false)"으로 표시된다. 코호트 정의에서 고려된 개인의 건강 상태에 대한 절대적 진리는 알기 어렵지만, 기준이 되는 골드 스탠다드를 확립하는 방법은 여러 가지가 있는데, 그 중 두 가지는 이번 장 후반부에 기술할 것이다. 사용한 방법에 관계없이, 이러한 사람에 대한 라벨링은 코호트 정의에 설명된 것과 동일하다.

표현형(phenotype) 지정의 이항 표시 오류 외에도, 건강 상태의 타이밍도 부정확할 수 있다. 예를 들어, 코호트 정의는 어떤 사람이 특정 표현형에 속한다고 올바르게 정의할 수 있지만, 언제부터 해당 건강 조건을 갖게 되었는지 시점을 정하는 데에는 부정확할 수 있다. 이 오류는 생존 분석 결과(예: 위험비 hazard ratio)를 이용하는 분석에서 편향을 유발할 수 있다.

이 과정의 다음 단계는 코호트 정의와 골드 스탠다드와의 일치성을 평가하는 것이다. 골드 스탠다드와 코호트 정의에 의해 "양성"이라고 표기된 사람들을 "진양성(true positive)"이라고 부른다. 골드 스탠다드에 의해 "음성"로, 코호트 정의에 의해 "양성"으로 분류된 사람들을 "위양성(false positive)"이라고 부른다. 예를 들어, 코호트 정의가 실제 특정 상태가 없는 환자를 잘못하여 특정 상태가 있다고 판단할 수 있다. 골드 스탠다드와 코호트 정의에 의해 모두 "음성"으로 정의된 환자들은 "진음성(true negative)" 라고 부른다. 골드 스탠다드에 의해 "양성"으로 분류되었으나 코호트 정의에 의해 "음성"으로 분류된 환자들은 "위음성(false negative)"라고 부른다. 예를 들어 실제로는 환자가 특정 건강 상태를 지니고 있으나, 코호트 정의에서는 그렇지 않게 분류될 수 있다. 혼동 행렬의 네 칸의 숫자들을 세어, 우리는 코호트 정의가 사람들을 실제 표현형으로 분류하는 데 얼마나 정확한지 여부를 정량화할 수 있다. 다음은 코호트 정의 성능을 측정하기 위한 표준 성능 평가기준들이다.

1. **코호트 정의 민감도(sensitivity)** – 실제 표현형에 속하는 피험자 중 얼마나 많은 비율의 피험자들이 코호트 정의에 의해 분류되는가? 다음의 공식으로 구한다:

    민감도 = 진양성 / (진양성 + 위음성)

2.	**코호트 정의 특이도(specificity** – 실제 표현형에 속하지 않는 피험자 중 얼마나 많은 비율의 피험자들이 코호트 정의에 의해 음성으로 분류되는가? 다음의 공식으로 구한다:

    특이도 = 진음성 / (진음성 + 위양성)

3. **코호트 정의 양성 예측도(Positive predictive value, PPV)** – 코호트 정의에 의해 양성으로 분류되는 환자 중 표현형을 실제로 가지고 있는 환자가 얼마나 되는가? 다음의 공식으로 구한다:

    양성 예측도 = 진양성 / (진양성 + 위양성)

4. **코호트 정의 음성 예측도(Negative predictive value, NPV)** – 코호트 정의에 의해 음성으로 분류되는 환자 중 표현형을 실제로 가지고 있지 않은 환자가 얼마나 되는가? 다음의 공식으로 구한다:

    음성예측도 = 진음성 / (진음성 + 위음성)

위의 측정기준에서 만점은 100%이다. 관측 데이터의 특성상 만점은 보통 평균과 거리가 멀다. @Rubbo2015phenotypes 은 심근경색에 대한 코호트 정의를 검증하는 연구를 검토했다. 그들이 조사한 33개의 연구 중, 오직 하나의 데이터 집합에서 하나의 코호트 정의만이 양성 예측도 대해 만점을 얻었다. 전체적으로 33개 연구 중 31개에서 70% 이상의 양성 예측도가 보고되었다. 그러나 그들은 또한 33개 연구 중 11개만이 민감도를 보고했고 5개만이 특이도을 보고했다는 것을 발견했다. 양성예측도는 민감성, 특이성, 유병율의 함수다. 유병률에 대한 값이 다른 데이터에서는 민감도와 특이도가 일정하게 유지되더라도 양성예측도에 대해서는 다른 값을 생성한다. 민감도와 특이도가 없다면 불완전한 코호트 정의로 인한 편향을 수정할 수 없다. 게다가, 건강 상태의 오분류가 수행될 수 있는데, 즉 대상 및 대조 코호트 정의 수행 시 오분류의 정도가 비슷할 수도 있지만, 그 정도가 두 그룹 간에 매우 다를 수도 있다는 점이다. 이전의 코호트 정의 검증 연구는 실제 추정치에 강한 편향을 초래할 수 있음에도 불구하고 대상 및 대조 코호트 간의 오분류의 잠재적 가능성에 대한 시험을 하지 않았다.

코호트 정의에 대한 성능 평가기준이 마련되면, 이러한 정의를 사용하여 연구 결과를 조정하는 데 사용될 수 있습니다. 이론적으로, 이러한 측정 오차 추정치를 이용해 연구 결과를 보정하는 방법은 잘 확립되어 있다. 그러나 실제로는 성능 평가 점수를 얻기 어렵기 때문에, 이러한 보정은 거의 고려되지 않고 있다. 골드 스탠다드를 결정하는 방법은 이 절의 나머지 부분에 설명되어 있다.

## 원천 기록 검증

\index{source record verification}

코호트 정의를 검증하는 데 사용되는 일반적인 방법은 원천 기록 확인을 통한 임상적 판단이다. 즉 관심 임상 조건이나 특성을 충분히 분류할 수 있는 충분한 지식을 가진 하나 분야 이상의 전문가가 개인의 기록을 철저히 검사하는 것이다. 차트 검토는 보통 다음의 단계를 따른다:

1. 임상연구심의위원회 (institutional review board, IRB) 또는 환자 개인에게 직접 차트 검토 및 연구에 대한 승인을 받는다.
2. 평가할 코호트 정의를 사용하여 코호트를 생성한다. 수동으로 전체 코호트를 판단할 만큼 시간적, 인적 자원이 충분하지 않은 경우 검토 대상 중 일부를 표본으로 추출한다.
3. 환자 차트를 검토할 수 있는 충분한 임상 전문 지식을 가진 사람 한 명 이상을 섭외한다.
4. 환자가 원하는 임상 조건이나 특성을 가지고 있는지에 대해 양성 및 음성을 판단하기 위한 지침을 결정한다.
5. 임상 전문가는 각 환자가 표현형에 속하는지 여부를 분류하기 위해 표본 내의 사람에 대한 모든 가용 데이터를 검토하고 판단한다.
6. 코호트 정의 분류 및 임상 판정 분류에 따라 혼동 행렬로 도표화하고 수집된 데이터에서 가능한 성능을 평가한다.

차트 검토의 결과는 일반적으로 하나의 성능 지표인 양성예측도 평가로 제한된다. 평가 대상이 되는 코호트 정의는 원하는 조건이나 특성을 가진 것으로 생각는 사람만 생성하기 때문이다. 따라서 코호트의 표본에 있는 각 개인은 임상적 판단에 근거하여 진양성 또는 위양성 중 하나로 분류된다. 전체 모집단의 표현형(코호트 정의에 의해 식별되지 않은 사람을 포함)에 있는 모든 사람에 대한 지식이 없으면, 위음성의 식별이 불가능하며, 따라서 혼동행렬의 나머지 부분을 채워 나머지 성능 특성을 채울 수 없다. 모집단 전체의 표현형을 식별하는 가능한 방법에는 모집단이 작지 않은 경우 불가능한 전체 데이터에 대한 차트 검토 또는 이미 충분한 임상 정보와 특정 표현형 유무가 결정되어 있는 임상 레지스트리 (예: 암 레지스트리)의 활용 등이 포함되나 이는 일반적으로는 실행이 불가능하다 (아례 예 참조). 또는 코호트 정의에 적합하지 않은 사람을 표본으로 추출하여 예측된 음성 집단의 표본을 생성한 다음, 위 차트 검토의 3-6단계를 반복하여 이러한 환자가 진정으로 관심의 임상 조건이나 특성이 없었는지 여부를 확인할 수 있다. 이렇게 하면 음성예측도를 추정할 수 있으며, 표현형의 유병률에 대한 적절한 추정치를 구할 수 있다면, 민감도와 특이도를 추정할 수 있다.

원천 기록 확인, 차트 검토를 통한 임상적 판단에는 여러 가지 한계가 있다. 앞서 언급했듯이, 차트 검토는 양성예측도와 같은 단일 지표의 평가에도 매우 많은 시간이 소요되고 자원이 많이 소요되는 과정이 될 수 있다. 이러한 한계는 혼동 행렬을 완전히 채우기 위해서 전체 모집단을 평가해야 하기 때문에 실용성을 크게 저해한다. 또한 상기 프로세스의 다중 단계는 연구의 결과를 편향시킬 수 있는 잠재력을 가지고 있다. 예를 들어, EHR에서 기록에 동일하게 접근할 수 없거나, EHR이 없거나, 개별적인 환자 동의가 필요한 경우, 추출된 평가 대상의 표본은 정말로 무작위적이지 않을 수 있으며, 선택 편향을 야기할 수 있다. 또한 수동적인 프로세스는 인간의 실수나 잘못된 분류에 취약하므로 완벽하게 정확한 측정 기준을 나타내지 못할 수 있다. 환자의 기록이 불충분하거나, 결정이 주관적이거나, 충분한 전문성이 없는 등의 이유로 종종 임상 전문가들 사이에 의견 불일치가 있을 수 있다. 많은 연구에서, 전문가들 사이의 불일치에 대해 충분히 고려하지 않는 다수결의 원칙을 통해 일방적으로 해결된다.

### 원천 기록 검증에 대한 예시


차트 검토를 활용한 코호트 정의 유효성을 검사하는 과정의 예시는 CUIMC(Columbia University Irving Medical Center)에 수행하는 연구로부터 제공되어지는데, 그 연구는 NCI(National Cancer Institute)에 행하는 타당성 연구의 일환인 다수의 암에 대한 코호트 정의를 검수하는 것이다. 이 과정은 이러한 암 중 하나인 전립선 암을 검증하는데 사용되었는데, 과정은 다음과 같다.

1. 제안서를 제출하고, OHDSI 암 표현형 연구를 위한 IRB의 동의를 얻었다.
2.전립선 암에 대한 코호트 정의를 개발하였다: 어휘를 탐구하기 위해 ATHENA와 ATLAS를 사용해서, 우리는 전립선 2차 신경세포(concept ID 4314337) 또는 Non-Hodgkin’s의 전립선 림프종(concept ID 4048666)을 제외한 전립선 악성 종양(concept ID 4163261)을 가지고 있는 모든 환자를 포함한 코호트 정의를 생성했다.
3. ATLAS를 사용하여 코호트를 생성하고, 수동 검토를 위한 100명의 임의 환자를 지정하여 개개인의 PERSON_ID를 환자 MRN을 매핑테이블을 사용하여 매핑하였다. PPV의 성능 측정 기준에 맞는 원하는 통계적 정밀도를 달성하기 위하여 100명의 환자가 선정되었다.
4. 위의 임의로 선정한 각각의 환자가 참 혹은 거짓 양성 반응을 보이는지 밝히기 위하여 입원 환자와 외래 환자를 모두 포함한 다양한 EHRs의 기록을 수동으로 검토하였다.
5. 한 의사가 수동 검토와 임상 판정을 실행하였다. (비록 이상적으로는 미래에 합의와 계층간의 신뢰성을 평가하기 위해 더욱 엄격한 검증 연구가 더욱 많은 검토자에 의해 실행되어야 할 것이다.)
6. 참고 기준의 결정은 임상 문서, 병리학 보고서, 실험실, 의약품, 사용 가능한 전자 환자기록 전체 문서가 기반이 되었다.
7. 환자들은 1)전립선 암 2)비 전립선 암 3)인식불가 로 분류되었다.

8. A conservative estimate of PPV was calculated using the following: prostate cancer/ (no prostate cancer + unable to determine).
9. Then, using the tumor registry as an additional gold standard to identify a reference standard across the entire CUIMC population, we counted the number of persons in the tumor registry which were and were not accurately identified by the cohort definition, which allowed us to estimate sensitivity using these values as true positives and false negatives.
10. Using the estimated sensitivity, PPV, and prevalence, we could then estimate specificity for this cohort definition.
As noted previously, this process was time-consuming and labor-intensive, as each cohort definition had to be individually evaluated through manual chart review as well as correlated with the CUIMC tumor registry in order to identify all performance metrics. The IRB approval process itself took weeks despite an expedited review while obtaining access to the tumor registry, and the process of manual chart review itself took a few weeks longer.

A review of validation efforts for myocardial infarction (MI) cohort definitions by @Rubbo2015phenotypes found that there was significant heterogeneity in the cohort definitions used in the studies as well as in the validation methods and the results reported. The authors concluded that for acute myocardial infarction there is no gold standard cohort definition available. They noted that the process was both costly and time-consuming. Due to that limitation, most studies had small sample sizes in their validation leading to wide variations in the estimates for the performance characteristics. They also noted that in the 33 studies, while all the studies reported positive predictive value, only 11 studies reported sensitivity and only five studies reported specificity. As mentioned previously, without estimates of sensitivity and specificity, statistical correction for misclassification bias cannot be performed.

## PheValuator

\index{PheValuator}

The OHDSI community has developed a different approach to constructing a gold standard by using diagnostic predictive models. [@Swerdel2019phevaluator] The general idea is to emulate the ascertainment of the health outcome similar to the way clinicians would in a source record validation, but in an automated way that can be applied at scale. The tool has been developed as an open-source R package called PheValuator.[^phevaluatorUrl] PheValuator uses functions from the Patient Level Prediction package.

[^phevaluatorUrl]: https://github.com/OHDSI/PheValuator

The process is as follows:

1. Create an extremely specific (“**xSpec**”) cohort: Determine a set of persons with a very high likelihood of having the outcome of interest to be used as noisy positive labels when training a diagnostic predictive model.
2. Create an extremely sensitive (“**xSens**”) cohort: Determine a set of persons that should include anyone who could possible have the outcome. This cohort will be used to identify its inverse: the set of people we are confident do not have the outcome, to be used as noisy negative labels when training a diagnostic predictive model.
3. Fit a predictive model using the xSpec and xSens cohort: As described in Chapter \@ref(PatientLevelPrediction), we fit a model using a wide array of patient features as predictors, and aim to predict whether a person belongs to the xSpec cohort (those we believe have the outcome) or the inverse of the xSens cohort (those we believe do not have the outcome).
4. Apply the fitted model to estimate the probability of the outcome for a hold-out set of persons who will be used to evaluate cohort definition performance: The set of predictors from the model can be applied to a person’s data to estimate the predicted probability that the person belongs to the phenotype. We use these predictions as a **probabilistic gold standard**.
5. Evaluate the performance characteristics of the cohort definitions: We compare the predicted probability to the binary classification of a cohort definition (the test conditions for the confusion matrix). Using the test conditions and the estimates for the true conditions, we can fully populate the confusion matrix and estimate the entire set of performance characteristics, i.e., sensitivity, specificity, and predictive values.

The primary limitation to using this approach is that the estimation of the probability of a person having the health outcome is limited by the data in the database. Depending on the database, important information, such as clinician notes, may not be available.

In diagnostic predictive modeling we create a model that discriminates between those with the disease and those without the disease. As described in the Patient-Level Prediction chapter (Chapter \@ref(PatientLevelPrediction)), prediction models are developed using a *target cohort* and an *outcome cohort*. The target cohort includes persons with and without the health outcome; the outcome cohort identifies those persons in the target cohort with the health outcome. For the PheValuator process, we use an extremely specific cohort definition, the “xSpec” cohort, to determine the outcome cohort for the prediction model. The xSpec cohort uses a definition to find those with a very high probability of having the disease of interest. The xSpec cohort may be defined as those persons who have multiple condition occurrence records for the health outcome of interest. For example, for atrial fibrillation, we may have persons who have 10 or more records with the atrial fibrillation diagnosis code. For MI, an acute outcome, we may use 5 occurrences of MI and include the requirement of having at least two occurrences from an inpatient setting. The target cohort for the predictive model is constructed from the union of persons with a low likelihood of having the health outcome of interest and those persons in the xSpec cohort. To determine those persons with a low likelihood of having the health outcome of interest, we sample from the entire database and exclude persons who have some evidence suggestive of belonging to the phenotype, typically by removing persons with any records containing the concepts used to define the xSpec cohort. There are limitations to this method. It is possible that these xSpec cohort persons may have different characteristics than others with the disease. It may also be that these persons had longer observation time after initial diagnosis than the average patient. We use LASSO logistic regression to create the prediction model used to generate the probabilistic gold standard. [@suchard_2013] This algorithm produces a parsimonious model and typically removes many of the collinear covariates which may be present across the dataset. In the current version of the PheValuator software, outcome status (yes/no) is evaluated based on all data for a person (all observation time), and does not evaluate the accuracy of the cohort start date.

### Example Validation By PheValuator

We may use PheValuator to assess the complete performance characteristics for a cohort definition to be used in a study where it is necessary to determine those persons who have had an acute myocardial infarction.

The following are the steps for testing cohort definitions for MI using PheValuator:

#### Step 1: Define the xSpec Cohort {-}

Determine those with MI with a high probability. We required a condition occurrence record with a concept for myocardial infarction or any of its descendants, with one or more occurrences of MI recorded from a hospital in-patient visit within 5 days, and 4 or more occurrences of MI in the patient record within 365 days. Figure \@ref(fig:xSpec) illustrates this cohort definition for MI in ATLAS. \index{xSpec cohort}

```{r xSpec, fig.cap='An extremely specific cohort definition (xSpec) for myocardial infarction.', echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ClinicalValidity/xSpec.png")
```

#### Step 2: Define the xSens Cohort {-}

We then develop an extremely sensitive cohort (xSens). This cohort may be defined for MI as those persons with at least one condition occurrence record containing a myocardial infarction concept at any time in their medical history. Figure \@ref(fig:xSens) illustrates the xSens cohort definition for MI in ATLAS. \index{xSens cohort}

```{r xSens, fig.cap='An extremely sensitive cohort definition (xSens) for myocardial infarction.', echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ClinicalValidity/xSens.png")
```

#### Step 3: Fit the Predictive Model {-}

The function `createPhenoModel` develops the diagnostic predictive model for assessing the probability of having the health outcome of interest in the evaluation cohort. To use this function, we utilize the xSpec and xSens cohorts developed in Steps 1 and 2. The xSpec cohort will be entered as the `xSpecCohort` parameter in the function. The xSens cohort will be entered as the `exclCohort` parameter in the function to indicate that those in the xSens cohort should be excluded from the target cohort used in the modeling process. Using this exclusion method, we can determine persons with a low likelihood of having the health outcome. We may think of this group as “noisy negative” persons, i.e., a group of persons likely negative for the health outcome but allowing for a small possibility of including some persons positive for the health outcome. We may also use the xSens cohort as the `prevCohort` parameter in the function. This parameter is used in the process to determine an approximate prevalence of the health outcome in the population. Normally, a large random sample of persons from a database should produce a population of persons where the persons with the outcome of interest are about in proportion to the prevalence of the outcome in the database. Using the method we described, we no longer have a random sample of persons and need to re-calibrate the predictive model based on resetting the proportion of persons with the outcome to those without the outcome.

All concepts used to define the xSpec cohort must be excluded from the modeling process. To do this we set the `excludedConcepts` parameter to the list of concepts used in the xSpec definition. For example, for MI we created a concept set in ATLAS using the concept for Myocardial infarction plus all its descendants. For this example, we would set the excludedConcepts parameter to 4329847, the concept Id for Myocardial infarction, and we would also set the addDescendantsToExclude parameter to TRUE, indicating that any descendants of the excluded concepts should also be excluded.

There are several parameters that may be used to specify the characteristics of the persons included in the modeling process. We can set the ages of the persons included in the modeling process by setting the `lowerAgeLimit` to the lower bounds of age desired in the model and the `upperAgeLimit` to the upper bounds. We may wish to do this if the cohort definitions for a planned study will be created for a certain age group. For example, if the cohort definition to be used in a study is for Type 1 diabetes mellitus in children, you may want to limit the ages used to develop the diagnostic predictive model to ages 5 to 17 years old for example. In doing so, we will produce a model with features that are likely more closely related to the persons selected by the cohort definitions to be tested. We can also specify which sex is included in the model by setting the `gender` parameter to the concept ID for either male or female. By default, the parameter is set to include both males and females. This feature may be useful in sex-specific health outcomes such as prostate cancer. We can set the time frame for person inclusion based on the first visit in the person’s record by setting the `startDate` and `endDate` parameters to the lower and upper bounds of the date range, respectively. Finally, the `mainPopnCohort` parameter may be used to specify a large population cohort from which all persons in the target and outcome cohorts will be selected. In most instances this will be set to 0, indicating no limitation on selecting persons for the target and outcome cohorts. There may be times, however, when this parameter is useful for building a better model, possibly in cases where the prevalence of the health outcome is extremely low, perhaps 0.01% or lower. For example:

```{r tidy=FALSE, eval=FALSE}
setwd("c:/temp")
library(PheValuator)
connectionDetails <- createConnectionDetails(
  dbms = "postgresql",
  server = "localhost/ohdsi",
  user = "joe",
  password = "supersecret")

phenoTest <- createPhenoModel(
  connectionDetails = connectionDetails,
  xSpecCohort = 10934,
  cdmDatabaseSchema = "my_cdm_data",
  cohortDatabaseSchema = "my_results",
  cohortDatabaseTable = "cohort",
  outDatabaseSchema = "scratch.dbo", #should have write access
  trainOutFile = "5XMI_train",
  exclCohort = 1770120, #the xSens cohort
  prevCohort = 1770119, #the cohort for prevalence determination
  modelAnalysisId = "20181206V1",
  excludedConcepts = c(312327, 314666),
  addDescendantsToExclude = TRUE,
  cdmShortName = "myCDM",
  mainPopnCohort = 0, #use the entire person population
  lowerAgeLimit = 18,
  upperAgeLimit = 90,
  gender = c(8507, 8532),
  startDate = "20100101",
  endDate = "20171231")
```

In this example, we used the cohorts defined in the “my_results” database, specifying the location of the cohort table (cohortDatabaseSchema, cohortDatabaseTable - “my_results.cohort”) and where the model will find the conditions, drug exposures, etc. to inform the model (cdmDatabaseSchema - “my_cdm_data”). The persons included in the model will be those whose first visit in the CDM is between January 1, 2010 and December 31, 2017. We are also specifically excluding the concept IDs 312327, 314666, and their descendants which were used to create the xSpec cohort. Their ages at the time of first visit will be between 18 and 90. With the parameters above, the name of the predictive model output from this step will be: “c:/temp/lr_results_5XMI_train_myCDM_ePPV0.75_20181206V1.rds”

#### Step 4: Creating the Evaluation Cohort {-}

The function `createEvalCohort` uses the PatientLevelPrediction package function `applyModel` to produce a large cohort of persons, each with a predicted probability for the health outcome of interest. The function requires specifying the xSpec cohort (by setting the `xSpecCohort` parameter to the xSpec cohort ID). We may also specify the characteristics of the persons included in the evaluation cohort as we did in the previous step. This could include specifying the lower and upper ages limits (by setting, as ages, the `lowerAgeLimit` and `upperAgeLimit` arguments, respectively), the sex (by setting the `gender` parameter to the concept IDs for male and/or female), the starting and ending dates (by setting, as dates, the `startDate` and `endDate` arguments, respectively), and designating a large population from which to select the persons by setting the `mainPopnCohort` to the cohort Id for the population to use.

For example:

```{r tidy=FALSE, eval=FALSE}
setwd("c:/temp")
connectionDetails <- createConnectionDetails(
  dbms = "postgresql",
  server = "localhost/ohdsi",
  user = "joe",
  password = "supersecret")

evalCohort <- createEvalCohort(
  connectionDetails = connectionDetails,
  xSpecCohort = 10934,
  cdmDatabaseSchema = "my_cdm_data",
  cohortDatabaseSchema = "my_results",
  cohortDatabaseTable = "cohort",
  outDatabaseSchema = "scratch.dbo",
  testOutFile = "5XMI_eval",
  trainOutFile = "5XMI_train",
  modelAnalysisId = "20181206V1",
  evalAnalysisId = "20181206V1",
  cdmShortName = "myCDM",
  mainPopnCohort = 0,
  lowerAgeLimit = 18,
  upperAgeLimit = 90,
  gender = c(8507, 8532),
  startDate = "20100101",
  endDate = "20171231")
```

In this example, the parameters specify that the function should use the model file: “c:/temp/lr_results_5XMI_train_myCDM_ePPV0.75_20181206V1.rds”
to produce the evaluation cohort file: “c:/temp/lr_results_5XMI_eval_myCDM_ePPV0.75_20181206V1.rds”
The model and the evaluation cohort files created in this step will be used in the evaluation of the cohort definitions provided in the next step.

#### Step 5: Creating and Testing Cohort Definitions {-}

The next step is to create and test the cohort definitions to be evaluated. The desired performance characteristics may depend on the intended use of the cohort to address the research question of interest. For certain questions, a very sensitive algorithm may be required; others may require a more specific algorithm. The process for determining the performance characteristics for a cohort definition using PheValuator is shown in Figure \@ref(fig:phevaluatorDiagram).

```{r phevaluatorDiagram, fig.cap='Determining the Performance Characteristics of a cohort definition using PheValuator. p(O) = Probability of outcome; TP = True Positive; FN = False Negative; TN = True Negative; FP = False Positive.', echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ClinicalValidity/PheValuatorEvaluation.png")
```

In part A of Figure \@ref(fig:phevaluatorDiagram), we examined the persons from the cohort definition to be tested and found those persons from the evaluation cohort (created in the previous step) who were included in the cohort definition (Person IDs 016, 019, 022, 023, and 025) and those from the evaluation cohort who were not included (Person Ids 017, 018, 020, 021, and 024). For each of these included/excluded persons, we had previously determined the probability of the health outcome using the predictive model (p(O)).

We estimated the values for True Positives, True Negatives, False Positives, and False Negatives as follows (Part B of Figure \@ref(fig:phevaluatorDiagram)):

1. If the cohort definition included a person from the evaluation cohort, i.e., the cohort definition considered the person a “positive.” The predicted probability for the health outcome indicated the expected value of the number of counts contributed by that person to the True Positives, and one minus the probability indicated the expected value of the number of counts contributed by that person to the False Positives for that person. We added all the expected values of counts across persons to get the total expected value. For example, PersonId 016 had a predicted probability of 99% for the presence of the health outcome, 0.99 was added to the True Positives (expected value of counts added 0.99) and 1.00–0.99 = 0.01 was added to the False Positives (0.01 expected value). This was repeated for all the persons from the evaluation cohort included in the cohort definition (i.e., PersonIds 019, 022, 023, and 025).

2. Similarly, if the cohort definition did not include a person from the evaluation cohort, i.e. the cohort definition considered the person a “negative,” one minus the predicted probability for the phenotype for that person was the expected value of counts contributed to True Negatives and was added to it, and, in parallel, the predicted probability for the phenotype was the expected value of counts contributed to the False Negatives and was added to it. For example, PersonId 017 had a predicted probability of 1% for the presence of the health outcome (and, correspondingly, 99% for the absence of the health outcome) and 1.00 – 0.01 = 0.99 was added to the True Negatives and 0.01 was added to the False Negatives. This was repeated for all the persons from the evaluation cohort not included in the cohort definition (i.e., PersonIds 018, 020, 021, and 024).

After adding these values over the full set of persons in the evaluation cohort, we filled the four cells of the confusion matrix with the expected values of counts for each cell, and we were able to create point estimates of the PA performance characteristics like sensitivity, specificity, and positive predictive value (Figure 1C). We emphasize that these expected cell counts cannot be used to assess the variance of the estimates, only the point estimates. In the example, the sensitivity, specificity, PPV, and NPV were 0.99, 0.63, 0.42, and 0.99, respectively.

Determining the performance characteristics of the cohort definition uses the function `testPhenotype`. This function uses the output from the prior two steps where we created the model and evaluation cohorts. We would set the modelFileName parameter to the RDS file output from createPhenoModel function, in this example, “c:/temp/lr_results_5XMI_train_myCDM_ePPV0.75_20181206V1.rds”. We would set the resultsFileName parameter to the RDS file output from createEvalCohort function, in this example, “c:/temp/lr_results_5XMI_eval_myCDM_ePPV0.75_20181206V1.rds”. To test the cohort definition we wish to use in our study, we set the `cohortPheno` to the cohort ID for that cohort definition. We can set the `phenText` parameter to some human readable description for the cohort definition, such as “MI Occurrence, Hospital In-Patient Setting”. We will set the `testText` parameter to some human readable description for the xSpec definition, such as “5 X MI.” The output from this step is a data frame that contains the performance characteristics for the cohort definition tested. The settings for the `cutPoints` parameter is a list of values that will be used to develop the performance characteristics results. The performance characteristics are usually calculated using the “expected values” as described in Figure 1. To retrieve the performance characteristics based on the expected values, we include “EV” in the list for the `cutPoints` parameter. We may also want to see the performance characteristics based on specific predicted probabilities, i.e., cut points. For example, if we wanted to see the performance characteristics of all those at or above a predicted probability of 0.5 were considered positive for the health outcome and all those under a predicted probability of 0.5 were considered negative, we would add “0.5” to the `cutPoints` parameter list. For example:

```{r tidy=FALSE, eval=FALSE}
setwd("c:/temp")
connectionDetails <- createConnectionDetails(
  dbms = "postgresql",
  server = "localhost/ohdsi",
  user = "joe",
  password = "supersecret")

phenoResult <- testPhenotype(
  connectionDetails = connectionDetails,
  cutPoints = c(0.1, 0.2, 0.3, 0.4, 0.5, "EV", 0.6, 0.7, 0.8, 0.9),
  resultsFileName =
    "c:/temp/lr_results_5XMI_eval_myCDM_ePPV0.75_20181206V1.rds",
  modelFileName =
    "c:/temp/lr_results_5XMI_train_myCDM_ePPV0.75_20181206V1.rds",
  cohortPheno = 1769702,
  phenText = "All MI by Phenotype 1 X In-patient, 1st Position",
  order = 1,
  testText = "MI xSpec Model - 5 X MI",
  cohortDatabaseSchema = "my_results",
  cohortTable = "cohort",
  cdmShortName = "myCDM")
```

In this example, a wide range of prediction thresholds are provided (cutPoints) including the expected value (“EV”). Given that parameter setting, the output from this step will provide performance characteristics (i.e, sensitivity, specificity, etc.) at each prediction threshold as well as those using the expected value calculations. The evaluation uses the prediction information for the evaluation cohort developed in the prior step. The data frames produced from this step may be saved to a csv file for detailed examination.

Using this process, Table \@ref(tab:phevalStats) displays the performance characteristics for four cohort definitions for MI across five datasets. For a cohort definition similar to the one evaluated by Cutrona and colleagues, “>=1 X HOI, In-Patient”, we found a mean PPV of 67% (range: 59%-74%).

Table: (\#tab:phevalStats) Performance characteristics of four cohort definitions using diagnostic condition codes to determine myocardial infarction on multiple datasets using pheValuator. Sens – Sensitivity ; PPV – Positive Predictive Value ; Spec – Specificity; NPV – Negative Predictive Value; Dx Code – Diagnosis code for the cohort.

| Phenotype   Algorithm                        | Database  | Sens  | PPV   | Spec  | NPV   |
|:---------------------------- |:--------- |:-----:|:-----:|:-----:|:-----:|
| >=1 X   HOI | CCAE      | 0.761 | 0.598 | 0.997 | 0.999 |
|             | Optum1862 | 0.723 | 0.530 | 0.995 | 0.998 |
|             | OptumGE66 | 0.643 | 0.534 | 0.973 | 0.982 |
|             | MDCD      | 0.676 | 0.468 | 0.990 | 0.996 |
|             | MDCR      | 0.665 | 0.553 | 0.977 | 0.985 |
| >= 2 X   HOI| CCAE      | 0.585 | 0.769 | 0.999 | 0.998 |
|             | Optum1862 | 0.495 | 0.693 | 0.998 | 0.996 |
|             | OptumGE66 | 0.382 | 0.644 | 0.990 | 0.971 |
|             | MDCD      | 0.454 | 0.628 | 0.996 | 0.993 |
|             | MDCR      | 0.418 | 0.674 | 0.991 | 0.975 |
| >=1 X   HOI, In-Patient| CCAE      | 0.674 | 0.737 | 0.999 | 0.998 |
|             | Optum1862 | 0.623 | 0.693 | 0.998 | 0.997 |
|             | OptumGE66 | 0.521 | 0.655 | 0.987 | 0.977 |
|             | MDCD      | 0.573 | 0.593 | 0.995 | 0.994 |
|             | MDCR      | 0.544 | 0.649 | 0.987 | 0.980 |
| 1 X HOI, In-Patient, 1st Position | CCAE      | 0.633 | 0.788 | 0.999 | 0.998 |
|             | Optum1862 | 0.581 | 0.754 | 0.999 | 0.997 |
|             | OptumGE66 | 0.445 | 0.711 | 0.991 | 0.974 |
|             | MDCD      | 0.499 | 0.666 | 0.997 | 0.993 |
|             | MDCR      | 0.445 | 0.711 | 0.991 | 0.974 |


## 근거의 일반화 {#GeneralizabilityOfEvidence}

코호트는 특정 관찰 데이터베이스의 맥락 안에서 잘 정의되고 완전히 평가될 수 있지만, 임상 타당성은 그 결과가 관심 대상 모집단에 일반화될 수 있다고 간주되는 정도에 의해 제한된다. 동일한 주제에 대한 복수의 관찰형 연구는 다른 결과를 산출할 수 있는데, 이는 설계와 분석 방법뿐만 아니라 데이터 출처의 선택에도 의해 발생할 수 있다. @madigan_2013 은 데이터베이스의 선택이 관찰 연구의 결과에 영향을 미친다는 것을 보여주었다. 그들은 10개의 관찰형 데이터베이스에 걸쳐 53개의 약물-결과 쌍과 2개의 연구 설계(코호트 연구 설계 및 자기 대조군 환자 연구 설계)에 대한 결과에서 이질성을 체계적으로 조사했다. 연구 설계를 일정하게 유지했음에도 불구하고, 실제 추정치의 상당한 이질성이 관찰되었다.

OHDSI 네트워크의 관찰형 데이터베이스들은 그들이 반영하는 인구 집단(예 : 소아 대 노인, 사보험 가입자 대 공공보험 가입자), 데이터가 수집되는 치료 환경(예 : 입원 환자 대 외래 환자, 1차 의료기관 대 2차, 특수 의료기관), 데이터 수집 프로세스(예 : 행정 청구, EHR, 임상 레지스트리) 및 치료의 기반이되는 국가 및 지역 보건 시스템 등에서 상당한 차이가 있다. 이러한 차이는 질병과 의료 개입의 영향을 연구할 때 관찰되는 결과의 이질성으로 나타날 수 있으며 네트워크 연구를 위한 각 데이터베이스의 품질에 대한 신뢰도에 영향을 미칠 수도 있다. OHDSI 네트워크 내의 모든 데이터베이스는 CDM으로 표준화되지만, 표준화가 모집단 전체에 존재하는 진정한 고유의 이질성을 없애기 위함이 아니라, 네트워크 전체의 이질성을 인식하고 더 잘 이해하기 위한 일관된 프레임워크 제공을 강화한다는 점이 중요하다. OHDSI 연구 네트워크는 전 세계의 다양한 데이터베이스에 동일한 분석 프로세스를 적용할 수 있는 환경을 제공하기 때문에 연구자들은 다른 방법론적 측면을 일정하게 유지하면서 여러 데이터 베이스들에 걸쳐 결과를 해석할 수 있다. OHDSI의 커뮤니티의 오픈 사이언스에 대한 협력적 접근은 임상 영역의 전문가와 방법론적 분석 전문가들이 함께 OHDSI 네트워크의 데이터에 걸쳐 임상적 타당성을 이해하는 집단 지성에 도달하기 위한 한가지 방법이며, 이는 이 데이터들을 이용하여 만들어진 근거의 신뢰를 구축하기 위한 근본이 되어야 한다.

## 요약

```{block2, type='rmdsummary'}
- 임상적 타당성은 원천 데이터의 특성을 이해하고, 분석 내의 코호트 정의에 대한 성능을 평가하고, 대상 모집단에 대한 연구의 일반성을 평가함으로써 확립할 수 있다.
- 코호트 정의 및 가용한 관찰형 데이터에 근거하여 식별된 개인이 의도한 표현형에 진정으로 속하는 지 여부를 판단하여 코호트 정의의 성능을 평가할 수 있다.
- 코호트 정의 타당성 검사를 위해서는 측정 오류를 완전히 요약하고 조정할 수 있도록 민감도, 특이도, 양성예측도를 포함한 여러 성능 지표를 추정해야 한다.
- 원천 기록 확인을 통한 임상적 판단 및 PheValuator는 코호트 정의 타당성 검사에 대한 두 가지 대안적 접근 방식이다.
- OHDSI 네트워크 연구는 데이터 베이스들의 이질성을 평가하고 연구 결과의 일반성을 확장하여 실세계 근거의 임상적 타당성을 향상시키는 메커니즘을 제공한다.

```
