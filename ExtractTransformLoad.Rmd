# 추출 변환 적재 {#ExtractTransformLoad}

# Extract Transform Load {#ExtractTransformLoad}

*챕터 작성자 : Clair Blacketer & Erica Voss*
*번역 : 박지명*

*Chapter leads: Clair Blacketer & Erica Voss*

## 서론

## Introduction

원천 데이터에서 OMOP 공통 데이터 모델(Common Data Model, CDM)을 얻기 위해서는 추출, 변환, 적재(Extract Transform Load, ETL) 절차가 필요하다. 이 절차는 데이터를 CDM으로 재구축 과정이며, 표준용어로의 매핑, SQL 코드들을 이용한 자동화된 절차로 이루어지게 된다. ETL 절차는 원천 데이터가 갱신될 때마다 언제든지 재수행 가능하게끔 반복 가능하게 구축하는 것이 중요하다. \index{ETL|see {extract, transform and load (ETL)}} \index{raw data} \index{native data|see {raw data}} \index{source data|see{raw data}}

In order to get from the native/raw data to the OMOP Common Data Model (CDM) we have to create an extract, transform, and load (ETL) process. This process should restructure the data to the CDM, and add mappings to the Standardized Vocabularies, and is typically implemented as a set of automated scripts, for example SQL scripts. It is important that this ETL process is repeatable, so that it can be rerun whenever the source data is refreshed. \index{ETL|see {extract, transform and load (ETL)}} \index{raw data} \index{native data|see {raw data}} \index{source data|see{raw data}}

ETL을 진행한다는 것은 많은 일들을 필요로 한다. 몇 년 동안의 과정을 통해 우리는 4가지 단계들로 이루어진 최상의 단계들을 개발하였다.

Creating an ETL is usually a large undertaking. Over the years, we have developed best practices, consisting of four major steps:

1. 데이터 전문가와 CDM 전문가가 함게 ETL을 설계할 것.
2. 의학 지식이 있는 사람들이 코드 매핑을 작업할 것.
3. 기술자가 ETL을 수행할 것.
4. 모든 사람이 질 관리에 참여할 것.

1. Data experts and CDM experts together design the ETL.
2. People with medical knowledge create the code mappings.
3. A technical person implements the ETL.
4. All are involved in quality control.

이 챕터에서 우리는 각 단계들을 세부적으로 살펴볼 것이다. 각 절차들을 보조하기 위해 다양한 도구들이 OHDSI 커뮤니티에 의해 개발되어왔고, 이 도구들에 대해서도 다룰 것이다. 마지막으로 CDM과 ETL의 유지에 관해 이야기하며 마무리할 것이다.

In this chapter we will discuss each of these steps in detail. Several tools have been developed by the OHDSI community to support some of these steps, and these will be discussed as well. We close this chapter with a discussion of CDM and ETL maintenance.

## 1단계 : ETL 설계

## Step 1: Design the ETL

ETL 설계와 ETL 수행을 명확하게 분리하는 것은 중요하다. ETL을 설계하는 것은 원천 데이터와 CDM 모두에 대한 넓은 지식을 필요로 한다. 반대로 ETL을 수행할 때는 ETL을 기술적인 측면에서 효율적으로 수행하는 방법에 대해 기술 전문가들에게 의존하게 된다. 만약 동시에 두 가지 모두를 진행하려 한다면, 전체적인 그림에 집중할 때보다 세부적인 사항에서 막히게 될 가능성이 높다.

It is important to clearly separate the design of the ETL from the implementation of the ETL. Designing the ETL requires extensive knowledge of both the source data, as well as the CDM. Implementing the ETL on the other hand typically relies mostly on technical expertise on how to make the ETL computationally efficient. If we try to do both at once, we are likely to get stuck in nitty-gritty details, while we should be focusing on the overall picture.

두 가지의 밀접하게 연관된 도구들이 ETL 설계를 위해 개발되었다: 흰 토끼 (White Rabbit), 모자 속 토끼 (Rabbit-in-a-Hat)

Two closely-integrated tools have been developed to support the ETL design process: White Rabbit, and Rabbit-in-a-Hat.

### 흰 토끼 (White Rabbit)

### White Rabbit

ETL 절차를 시작하기 위해서는 테이블, 필드, 내용을 포함한 데이터에 대한 이해가 필요하다. 하단의 링크에 [White Rabbit](https://github.com/OHDSI/WhiteRabbit)에 대한 정보가 기록되어 있다. White Rabbit은 장기적인 보건의료 데이터베이스의 [OMOP CDM](https://github.com/OHDSI/CommonDataModel)으로의 ETL 작업 준비를 도와주기 위한 소프트웨어이다. White Rabbit은 데이터를 탐색하고 ETL 설계를 시작하기 위한 필수적인 정보들에 대한 보고서를 생성해준다. 모든 소스 코드와 설치 방법 뿐만 아니라 설명서는 깃헙(Github)에서 확인 가능하다.[^whiteRabbitGithubUrl] \index{White Rabbit} \index{data profiling|see {White Rabbit}}

To initiate an ETL process on a database you need to understand your data, including the tables, fields, and content. This is where the [White Rabbit](https://github.com/OHDSI/WhiteRabbit) tool comes in. White Rabbit is a software tool to help prepare for ETLs of longitudinal healthcare databases into the [OMOP CDM](https://github.com/OHDSI/CommonDataModel). White Rabbit scans your data and creates a report containing all the information necessary to begin designing the ETL. All source code and installation instructions, as well as a link to the manual, are available on GitHub.[^whiteRabbitGithubUrl] \index{White Rabbit} \index{data profiling|see {White Rabbit}}

[^whiteRabbitGithubUrl]: https://github.com/OHDSI/WhiteRabbit.

#### 범위와 목표 {-}

#### Scope and Purpose  {-}

White Rabbit의 주요 기능은 원천 데이터에 대한 탐색을 수행하고, 테이블, 필드, 필드값들에 대한 세부적인 정보를 제공하는 것이다. 원천 데이터는 comma-seperated 텍스트 파일일 수도 있고, 데이터베이스(MySQL, SQL Server, Oracle, PostgreSQL, Microsoft APS, Microsoft Access, Amazon RedShift)에 적재되어 있을 수도 있다. 탐색 과정에서  Rabbit-In-a-Hat 도구와 함께 쓴다면 ETL을 설계할 때 참고할 수 있는 보고서를 생성할 수 있다. White Rabbit은 다른 표준 데이터 프로파일링 도구들과는 달리 개인 식별 정보(Personally Identifiable Information, PII)가 결과 데이터 파일에서 보여지는 것을 방지한다.

White Rabbit’s main function is to perform a scan of the source data, providing detailed information on the tables, fields, and values that appear in a field. The source data can be in comma-separated text files, or in a database (MySQL, SQL Server, Oracle, PostgreSQL, Microsoft APS, Microsoft Access, Amazon RedShift). The scan will generate a report that can be used as a reference when designing the ETL, for instance by using it in conjunction with the Rabbit-In-a-Hat tool. White Rabbit differs from standard data profiling tools in that it attempts to prevent the display of personally identifiable information (PII) data values in the generated output data file.

#### 절차 개요 {-}

#### Process Overview {-}

원천 데이터를 탐색하기 위해 소프트웨어를 사용하는 일반적인 순서:

The typical sequence for using the software to scan source data:

1. 로컬 컴퓨터에서의 결과를 내보낼 작업 폴더를 설정.
2. 데이터베이스 혹은 CSV 텍스트 파일과의 연결 및 연결 확인.
3. 탐색 대상 테이블 선택 및 탐색.
4. White Rabbit의 원천 데이터에 대한 정보 생성 및 내보내기.

1. Set working folder, the location on the local desktop computer where results will be exported.
2. Connect to the source database or CSV text file and test connection.
3. Select the tables of interest for the scan and scan the tables.
4. White Rabbit creates an export of information about the source data.

#### 작업 폴더 설정 {-}

#### Setting a Working Folder {-}

White Rabbit 어플리케이션의 다운로드 및 설치 이후, 처음으로 할 일은 작업 폴더를 설정하는 것이다. White Rabbit이 생성하는 모든 파일들은 설정한 로컬 폴더에 생성될 것이다. 그림 \@ref(fig:WhiteRabbitLocation)에서 보여지는 "Pick Folder" 버튼을 사용하여 탐색 문서들이 저장될 로컬 환경을 탐색할 수 있다. 

After downloading and installing the White Rabbit application, the first thing you need to do is set a working folder. Any files that White Rabbit creates will be exported to this local folder. Use the "Pick Folder" button shown in Figure \@ref(fig:WhiteRabbitLocation) to navigate in your local environment where you would like the scan document to go. 

```{r WhiteRabbitLocation, fig.cap='The "Pick Folder" button allows the specification of a working folder for the White Rabbit application.',echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/WhiteRabbitLocation.png")
```

#### 데이터베이스 연결 {-}

#### Connection to a Database {-}

White Rabbit은 구분된 텍스트 파일들과 데이터베이스를 지원한다. 다양한 필드들에 대한 필요 항목들의 설명을 보려면 마우스를 올려야한다. 더욱 자세한 설명은 설명서에서 확인할 수 있다.

White Rabbit supports delimited text files and various database platforms. Hover the mouse over the various fields to get a description of what is required. More detailed information can be found in the manual.

#### 데이터베이스의 테이블 탐색 {-}

#### Scanning the Tables in a Database {-}

데이터베이스에 연결한 이후에는 데이터베이스에 적재되어있는 테이블들을 탐색할 수 있다. 탐색 과정은 ETL을 설계하는데 도움을 되는 원천 데이터에 대한 정보를 담은 보고서를 생성할 수 있다. 그림 \@ref(fig:WhiteRabbitAddTables)에 보이는  Scan 탭의 “Add” (Ctrl + mouse click) 버튼을 눌러서 선택된 원천 데이터베이스의 각 테이블들을 선택하거나, “Add all in DB” 누름으로써 모든 테이블들을 자동적으로 선택할 수 있다.

After connecting to a database, you can scan the tables contained therein. A scan generates a report containing information on the source data that can be used to help design the ETL. Using the Scan tab shown in Figure \@ref(fig:WhiteRabbitAddTables) you can either select individual tables in the selected source database by clicking on “Add” (Ctrl + mouse click), or automatically select all tables in the database by clicking on “Add all in DB”.

```{r WhiteRabbitAddTables, fig.cap='White Rabbit Scan tab.', echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/WhiteRabbitAddTables.png")
```

탐색에 사용될 몇 가지 옵션들:

There are a few setting options as well with the scan:

* “Scan field values” 는 컬럼에 어떠한 값들을 나타나는지 보고싶을때 사용한다.
* “Min cell count” 는 필드값을 탐색할 때 쓰이는 옵션이다. 기본값은 5로 설정되어 있으며, 이는 원천 데이터에서 5번 이하로 나타나는 값들은 보고서에 나타내지 않는 것을 의미한다. 각 데이터셋들은 각각의 고유한 규칙에 따라 minimal cell count가 정해져야할 것이다.
* “Rows per table” 는 필드값을 탐색할 때 쓰이는 옵션이다. 기본값으로 White Rabbit은 테이블에서 무작위로 100,000개의 행렬을 선택하여 탐색할 것이다. 

* Checking the “Scan field values” tells WhiteRabbit that you would like to investigate which values appear in the columns. 
* “Min cell count” is an option when scanning field values. By default, this is set to 5, meaning values in the source data that appear less than 5 times will not appear in the report. Individual data sets may have their own rules about what this minimum cell count can be. 
* “Rows per table” is an option when scanning field values. By default, White Rabbit will scan 100,000 randomly selected rows in the table. 

모든 옵션들이 설정된 이후에는 “Scan tables”을 누르면 된다. 탐색이 완료된 이후에는 보고서가 작업 폴더에 생성될 것이다. 

Once all settings are completed, press the “Scan tables” button. After the scan is completed the report will be written to the working folder.

#### 탐색 보고서의 이해 {-}

#### Interpreting the Scan Report {-}

탐색이 완료된 이후에는 선택된 작업 폴더에 엑셀 파일이 생성될 것이며, 엑셀 파일에는 스캔한 각 테이블에 대한 하나의 탭과 개요 탭이 있다. 개요 탭은 탐색한 모든 테이블들이며, 각 테이블의 필드, 각 필드의 데이터 타입, 필드의 최대 길이, 테이블의 행의 수, 탐색한 행의 수, 그리고 얼마나 많은 필드들이 비어있었는지 보여준다. 그림 \@ref(fig:ScanOverviewTab).은 개요 탭의 예시를 보여준다. 

Once the scan is complete, an Excel file is generated in the selected folder with one tab present for each table scanned as well as an overview tab. The overview tab lists all tables scanned, each field in each table, the data type of each field, the maximum length of the field, the number of rows in the table, the number of rows scanned, and how often each field was found to be empty. Figure \@ref(fig:ScanOverviewTab). shows an example overview tab.

```{r ScanOverviewTab, fig.cap="Example overview tab from a scan report.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/ScanOverviewTab.png")
```

각 테이블들의 탭들은 각각의 필드, 필드의 값들, 그리고 값들의 빈도를 나타낸다. 각 원천 테이블의 컬럼들은 엑셀에서 두 개의 컬럼들로 생성된다. 하나는 탐색 시 설정한 “Min cell count” 보다 큰 값들의 고유한 값을 보여준다. 만약 고유한 값 목록이 잘려있다면, 목록의 마지막 값은 “List truncated” 가 될 것이다; 이는 하나 혹은 그 이상의 값들이 “Min cell count” 보다 작은 고유한 값이 있음을 나타낸다. 각각의 고유한 값 옆에는 빈도를 나타내는 두 번째 컬럼이 있다(표본에서 값이 발생하는 횟수). 이 두 컬럼들(고유한 값과 빈도 수)은 작업책(workbook)의 프로파일링된 테이블의 모든 원천 변수들에 대해 반복되서 나타난다.

The tabs for each of the tables show each field, the values in each field, and the frequency of each value. Each source table column will generate two columns in the Excel. One column will list all distinct values that have a “Min cell count” greater than what was set at time of the scan. If a list of unique values was truncated, the last value in the list will be “List truncated”; this indicates that there are one or more additional unique source values that appear less than the number entered in the “Min cell count”. Next to each distinct value will be a second column that contains the frequency (the number of times that value occurs in the sample). These two columns (distinct values and frequency) will repeat for all the source columns in the table profiled in the workbook. 

```{r scanSex, fig.cap="Example values for a single column.", echo=FALSE, out.width='30%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/ScanSex.png")
```

보고서는 원천 데이터에 무엇이 있는지를 강조함으로써 데이터를 이해하는데 강력한 도움을 준다. 예를 들면, 그림 \@ref(fig:scanSex)에 나타난 결과가 탐색된 테이블 컬럼 중 하나인 "Sex"에 반환될 경우, 우리는 각각 61,491번과 35,401번 나타난 공통된 값들(1과 2)이 있음을 알 수 있다. White Rabbit은 1을 남성으로, 2를 여성으로 정의하지는 않을 것이다; 데이터 소유자가 일반적으로 원천 시스템에 고유한 원천 코드를 정의해야한다. 하지만 이 두 가지 값(1 & 2)들은 데이터에 있는 유일한 값들이 아니기 때문에 우리는 잘린 목록을 확인해야한다. 이 값들은 (“Min cell count” 정의에 따라) 매우 낮은 빈도로 나타나게 되고, 종종 부정확하거나 매우 의심스러운 값들로 표현된다. ETL 수행을 계획할 때 우리는 높은 빈도의 성별 개념로써 1과 2만 다루는 것이 아니라, 컬럼에 존재하는 낮은 빈도의 값들도 고려해야한다. 예를 들어 만약 낮은 빈도의 성별들이 “NULL”일 경우 ETL 진행 시 이러한 데이터에 대해 어떻게 처리할 것인지 확실히 해야한다. 

The report is powerful in understanding your source data by highlighting what exists. For example, if the results shown in Figure \@ref(fig:scanSex) were given back on the “Sex” column within one of the tables scanned, we can see that there were two common values (1 and 2) that appeared 61,491 and 35,401 times respectively. White Rabbit will not define 1 as male and 2 as female; the data holder will typically need to define source codes unique to the source system. However, these two values (1 & 2) are not the only values present in the data because we see this list was truncated. These other values appear with very low frequency (defined by “Min cell count”) and often represent incorrect or highly suspicious values. When generating an ETL we should not only plan to handle the high-frequency gender concepts 1 and 2 but the other low-frequency values that exist within this column. For example, if those lower frequency genders were “NULL” we want to make sure the ETL can handle processing that data and knows what to do in that situation. 

### 모자 속 토끼 (Rabbit-In-a-Hat)

### Rabbit-In-a-Hat

White Rabbit과 함께 우리는 원천 데이터에 대한 분명한 그림을 그릴 수 있다. 또한 우리는 CDM에 대한 전체 명세서를 알고 있다. 이제 우리는 하나에서 다른 하나로 넘어갈 로직을 정의해야한다. 이 설계 활동은 원천 데이터와 CDM 모두에 대한 온전한 지식을 요구한다. White Rabbit 소프트웨어와 함께 사용되는  Rabbit-in-a-Hat 도구는 명확하게 이 분야의 전문가들을 위해 개발되었다. 일반적으로 ETL 설계팀은 회의실에 같이 앉아 Rabbit-in-a-Hat을 프로젝터 화면으로 같이 보면서 작업을 한다. 첫 번째로 테이블 간의 매핑은 협력적으로 결정될 수 있으며, 그 후에는 필드 간의 매핑이 설계되는 동시에 어떤한 값들을 변환시킬지 로직을 정의할 수 있다. \index{Rabbit-In-A-Hat} \index{ETL design|see {Rabbit-In-A-Hat}}

With the White Rabbit scan in hand, we have a clear picture of the source data. We also know the full specification of the CDM. Now we need to define the logic to go from one to the other. This design activity requires thorough knowledge of both the source data and the CDM. The Rabbit-in-a-Hat tools that comes with the White Rabbit software is specifically designed to support a team of experts in these areas. In a typical setting, the ETL design team sits together in a room, while Rabbit-in-a-Hat is projected on a screen. In a first round, the table-to-table mappings can be collaboratively decided, after which field-to-field mappings can be designed, while defining the logic by which values will be transformed. \index{Rabbit-In-A-Hat} \index{ETL design|see {Rabbit-In-A-Hat}}

#### 범위와 목표 {-}

#### Scope and Purpose {-}

Rabbit-In-a-Hat은 White Rabbit의 탐색 문서들을 읽고 나타내기 위해 설계되었다. White Rabbit은 원천 데이터에 대한 정보를 생성하는반면, Rabbit-In-a-Hat은 그 정보를 사용하고 그래픽 사용자 인터페이스를 통하여 사용자들로 하여금 원천 데이터의 테이블과 컬럼들을 CDM으로 연결시게끔 해준다. Rabbit-In-a-Hat은 ETL 절차에 대한 문서를 생성해주지만 ETL을 위한 코드는 생성하지 않는다. 

Rabbit-In-a-Hat is designed to read and display a White Rabbit scan document. White Rabbit generates information about the source data while Rabbit-In-a-Hat uses that information and through a graphical user interface to allow a user to connect source data to tables and columns within the CDM. Rabbit-In-a-Hat generates documentation for the ETL process, it does not generate code to create an ETL.  

#### 절차 개요 {-}

#### Process Overview {-}

소프트웨어를 이용한 ETL 문서 생성을 위한 일반적인 순서:

1. WhiteRabbit의 완료된 탐색 결과.
2. 탐색 결과 확인; 인터페이스가 원천 테이블들과 CDM 테이블들을 보여줌.
3. 원천 테이블들의 정보와 상응하는 CDM 테이블들의 연결.
4. CDM 테이블에 상응하는 각 원천 테이블들에 대해서 세부적인 원천 컬럼과 CDM 컬럼의 연결을 정의.
5. Rabbit-In-a-Hat 작업을 저장하고 MS 워드 문서로 내보내기.

The typical sequence for using this software to generate documentation of an ETL:

1. Scanned results from WhiteRabbit completed.
2. Open scanned results; interface displays source tables and CDM tables.
3. Connect source tables to CDM tables where the source table provides information for that corresponding CDM table.
4. For each source table to CDM table connection, further define the connection with source column to CDM column detail.
5. Save Rabbit-In-a-Hat work and export to a MS Word document.

#### ETL 로직 작성 {-}

#### Writing ETL Logic {-}

일단 Rabbit-In-a-Hat 내의 White Rabbit 탐색 보고서를 확인한다면, 원천 데이터를 OMOP CDM으로 변환하는 설계와 로직 작성을 시작할 준비가 된다. 하나의 예시로써 하단의 챕터들에서 Synthea[^syntheaWiki] 데이터베이스의 일부 테이블들의 변환을 보여줄 것이다.

Once you have opened your White Rabbit scan report in Rabbit-In-a-Hat you are ready to begin designing and writing the logic for how to convert the source data to the OMOP CDM. As an example, the next few sections will depict how some of the tables in the Synthea[^syntheaWiki] database might look during conversion. 

[^syntheaWiki]: Synthea^TM^ is a patient generator that aims to model real patients. Data are created based on parameters passed to the application.The structure of the data can be found here: https://github.com/synthetichealth/synthea/wiki.

#### ETL의 일반적인 흐름 {-}

#### General Flow of an ETL {-}

CDM이 사람 중심의 모델이기 때문에 PERSON 테이블 먼저 매핑을 시작하는 것이 좋다. 모든 임상적 사건과 관련있는 테이블들(CONDITION_OCCURRENCE, DRUG_EXPOSURE, PROCEDURE_OCCURRENCE, etc.)은 PERSON 테이블의 person_id를 참조하기에 PERSON 테이블에 대한 로직을 먼저 작성하는 것이 나중을 위해 좋다. PERSON 테이블을 변환한 다음에는 OBSERVATION_PERIOD를 변환하는 것이 좋은 선택이다. CDM 데이터베이스의 각 사람들은 최소한 하나의 OBSERVATION_PERIOD를 가져야하고, 일반적으로 한 사람에 대한 모든 사건들은 이 관측시기 내에 맞춰지게 된다. PERSON과 OBSERVATION_PERIOD 테이블들이 완료되면 보통 PROVIDER, CARE_SITE, 그리고 LOCATION과 같은 디멘션 테이블(dimensional table)들이 다음 대상이 된다. 임상 테이블 이전에 마지막으로 로직을 작성해야하는 테이블은 VISIT_OCCURRENCE이다. 한 사람의 환자로써의 여정에서 대부분의 사건들이 방문할 때 발생하기 때문에 종종 모든 ETL 과정에서 가장 복잡하고 중요한 부분이기도 하다. 일단 이 테이블들이 완료되면 어떤 CDM 테이블들 어떤 순서대로 매핑할 지는 선택하기 나름이다.

Since the CDM is a person-centric model it is always a good idea to start mapping the PERSON table first. Every clinical event table (CONDITION_OCCURRENCE, DRUG_EXPOSURE, PROCEDURE_OCCURRENCE, etc.) refers to the PERSON table by way of the person_id so working out the logic for the PERSON table first makes it easier later on. After the PERSON table a good rule of thumb is to convert the OBSERVATION_PERIOD table next. Each person in a CDM database should have at least one OBSERVATION_PERIOD and, generally, most events for a person fall within this timeframe. Once the PERSON and OBSERVATION_PERIOD tables are done the dimensional tables like PROVIDER, CARE_SITE, and LOCATION are typically next. The final table logic that should be worked out prior to the clinical tables is VISIT_OCCURRENCE. Often this is the most complicated logic in the entire ETL and it is some of the most crucial since most events that occur during the course of a person’s patient journey will happen during visits. Once those tables are finished it is your choice which CDM tables to map and in which order.  

```{r etlFlow, fig.cap="General flow of an ETL and which tables to map first.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/flowOfEtl.png")
```

CDM 변환 과정에서 종종 중간 테이블들을 만들 필요가 있을 수 있다. 올바론 VISIT_OCCURRENCE_ID들을 사건들에 부여하거나 아니면 원천 코드를 표준 코드로 매핑하는 경우들일 수도 있다(이 단계는 종종 매우 느리게 진행된다). 중간 테이블들은 100% 허용되고 장려된다. 하지만 이러한 중간 테이블들이 변환이 완료된 이후에도 남아있거나 사용되는 것은 장려되지 않는다.

It is often the case that, during CDM conversion, you will need to make provisions for intermediate tables. This could be for assigning the correct VISIT_OCCURRENCE_IDs to events, or for mapping source codes to standard concepts (doing this step on the fly is often very slow). Intermediate tables are 100% allowed and encouraged. What is discouraged is the persistence and reliance on these intermediate tables once the conversion is complete. 

#### 매핑 예시: PERSON 테이블 {-}

#### Mapping Example: Person Table  {-}

Synthea 데이터 구조는 환자 테이블에 20개의 컬럼을 갖고 있지만 그림 \@ref(fig:syntheaPerson)에 보이는 것처럼 모든 컬럼이 PERSON 테이블에 필요한 것은 아니다. 이런 일은 매우 흔한 일이고 문제가 되지 않는다. 이 예시에서는 환자 이름, 운전면허번호, 여권 번호 등 Synthea의 환자 테이블의 많은 데이터 포인트들이 CDM PERSON 테이블에 사용되지 않는 것을 알 수 있다.

The Synthea data structure contains 20 columns in the patients table but not all were needed to populate the PERSON table, as seen in Figure \@ref(fig:syntheaPerson). This is very common and should not be cause for alarm. In this example many of the data points in the Synthea patients table that were not used in the CDM PERSON table were additional identifiers like patient name, driver’s license number, and passport number.

```{r syntheaPerson, fig.cap='Mapping of Synthea Patients table to CDM PERSON table.',echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/syntheaPersonTable.png")
```

하단의 표 \@ref(tab:syntheaEtlPerson)는 Synthea의 환자 테이블이 CDM PERSON 테이블로 변하는 로직을 보여준다. ‘Destination Field’는 CDM 데이터의 어디에 매핑되는지를 나타낸다. ‘Source field’는 원천 테이블(예시에서는 환자 테이블)의 어느 컬럼에서 CDM의 컬럼으로 변하는지를 나타낸다. 마지막으로, ‘Logic & comments’는 로직에 대한 설명을 의미한다. 

Table \@ref(tab:syntheaEtlPerson) below shows the logic that was imposed on the Synthea patients table to convert it to the CDM PERSON table. The ‘Destination Field’ discusses where in the CDM data is being mapped to. The ‘Source field’ highlights the column from the source table (in this case patients) that will be used to populate the CDM column. Finally, the ‘Logic & comments’ column gives explanations for the logic. 

Table: (\#tab:syntheaEtlPerson) ETL logic to convert the Synthea Patients table to CDM PERSON table.

| Destination Field | Source field | Logic & comments |
| :---------------------- | :--------- | :---------------------------------------- |
| PERSON_ID                   |             |  Autogenerate. The PERSON_ID will be generated at the time of implementation. This is because the id value from the source is a varchar value while the PERSON_ID is an integer. The id field from the source is set as the PERSON_SOURCE_VALUE to preserve that value and allow for error-checking if necessary. |
| GENDER_CONCEPT_ID           | gender      | When gender = ‘M’ then set GENDER_CONCEPT_ID to 8507, when gender = ‘F’ then set to 8532. Drop any rows with missing/unknown gender. These two concepts were chosen as they are the only two standard concepts in the gender domain. The choice to drop patients with unknown genders tends to be site-based, though it is recommended they are removed as people without a gender are excluded from analyses.|
| YEAR_OF_BIRTH               | birthdate   | Take year from birthdate |
| MONTH_OF_BIRTH              | birthdate   | Take month from birthdate |
| DAY_OF_BIRTH                | birthdate   | Take day from birthdate |
| BIRTH_DATETIME              | birthdate   | With midnight as time 00:00:00. Here, the source did not supply a time of birth so the choice was made to set it at midnight.  |
| RACE_CONCEPT_ID             | race        | When race = 'WHITE' then set as 8527, when race = 'BLACK' then set as 8516, when race = 'ASIAN' then set as 8515, otherwise set as 0. These concepts were chosen because they are the standard concepts belonging to the race domain that most closely align with the race categories in the source.  |
| ETHNICITY_ CONCEPT_ID        | race  ethnicity | When race = ‘HISPANIC’, or when ethnicity in (‘CENTRAL_AMERICAN’, ‘DOMINICAN’, ‘MEXICAN’, ‘PUERTO_RICAN’, ‘SOUTH_AMERICAN’) then set as 38003563, otherwise set as 0. This is a good example of how multiple source columns can contribute to one CDM column. In the CDM ethnicity is represented as either Hispanic or not Hispanic so values from both the source column race and source column ethnicity will determine this value. |
| LOCATION_ID                 |             |   |
| PROVIDER_ID                 |             |   |
| CARE_SITE_ID                |             |   |
| PERSON_SOURCE_ VALUE         | id          |   |
| GENDER_SOURCE_ VALUE         | gender      |   |
| GENDER_SOURCE_ CONCEPT_ID    |             |   |
| RACE_SOURCE_ VALUE           | race        |   |
| RACE_SOURCE_ CONCEPT_ID      |             |   |
| ETHNICITY_ SOURCE_VALUE      | ethnicity   |  In this case the ETHNICITY_SOURCE_VALUE will have more granularity than the ETHNICITY_CONCEPT_ID.  |
| ETHNICITY_ SOURCE_CONCEPT_ID |             |   |

Synthea 데이터의 CDM으로의 변환에 대한 더 자세한 설명은 전체 명세서를 참고하면 된다.[^syntheaEtlUrl]

For more examples on how the Synthea dataset was mapped to the CDM please see the full specification document.[^syntheaEtlUrl]

[^syntheaEtlUrl]: https://ohdsi.github.io/ETL-Synthea/

## 2 단계: 코드 매핑 생성

## Step 2: Create the Code Mappings


## 수정 필요
점점 더 많은 원천 코드들이 OMOP 용어에 추가되어지고 있다. 이것은 CDM으로 변환된 데이터의 코딩 체계가 이미 CDM에 포함되고 매핑되었을 수도 있다는 것을 의미한다. OMOP Vocabulary의 VOCABULARY 테이블을 통해 어떤 용어들이 포함되었는지 확인할 수 있다. non-standard인 원천 코드들(e.g. ICD-10CM codes)에서 standard concepts (e.g. SNOMED codes) 로의 매핑을 확인하려면 CONCEPT_RELATIONSHIP 테이블 내의 relationship_id = “Maps to” 인 값들을 찾으면 확인할 수 있다. 예를 들면 ICD-10CM 코드 ‘I21’ (“Acute Myocardial Infarction”)의 standard concept ID를 확인하기 위해 다음과 같은 SQL을 사용할 수 있다:

More and more source codes are being added to the OMOP Vocabulary all the time. This means that the coding systems in the data being transformed to the CDM may already be included and mapped. Check the VOCABULARY table in the OMOP Vocabulary to see which vocabularies are included. To extract the mapping from non-standard source codes (e.g. ICD-10CM codes) to standard concepts (e.g. SNOMED codes), we can use the records in the CONCEPT_RELATIONSHIP table having relationship_id = “Maps to”. For example, to find the standard concept ID for the ICD-10CM code ‘I21’ (“Acute Myocardial Infarction”), we can use the following SQL: 

```sql
SELECT concept_id_2 standard_concept_id
FROM concept_relationship
INNER JOIN concept source_concept
  ON concept_id = concept_id_1
WHERE concept_code = 'I21'
  AND vocabulary_id = 'ICD10CM'
  AND relationship_id = 'Maps to'; 
```
| STANDARD_CONCEPT_ID |
| -------------------:|
| 312327              |

하지만 안탑깝게도, 가끔은 원천 데이터가 Vocabulary에 없는 코딩 시스템을 사용할 수도 있다. 이러한 경우에는 원천 코딩 시스템의 Standard Concept으로의 매핑을 정의하여야한다. 하지만 원천 코딩 시스템에 많은 수의 용어들이 있을 경우 코드 매핑이 어려울 수도 있다. 이를 쉽게 진행하기 위한 몇 가지 참고사항들이 있다.

- 가장 높은 빈도의 코드들에 집중하라. 절대 쓰이지 않는 코드나 거의 안 쓰이는 코드들은 실제 연구에서도 안 쓰이기 때문에 많은 노력을 들여서 매핑을 진행할 필요가 없다.
- 가능하면 기존의 정보들을 활용하라. 예를 들어서 많은 국가 약물 코딩 시스템은 이미 ATC로 매핑되어있다. 비록 ATC가 많은 목적에 대해 세부적으로 부합하지는 않지만, ATC와 RxNorm의 관계를 통해 어떤 RxNorm 코드들이 사용되는지 추측할 수는 있다. 
- 우사기(Usagi)를 사용하라.

Unfortunately, sometimes the source data uses coding systems that are not in the Vocabulary. In this case, a mapping must be created from the source coding system to the Standard Concepts. Code mapping can be a daunting task, especially when there are many codes in the source coding system. There are several things that can be done to make the task easier: 

- Focus on the most frequently used codes. A code that is never used or infrequently used is not worth the effort of mapping, since it will never be used in a real study. 
- Make use of existing information whenever possible. For example, many national drug coding systems have been mapped to ATC. Although ATC is not detailed enough for many purposes, the concept relationships between ATC and RxNorm can be used to make good guesses of what the right RxNorm codes are. 
- Use Usagi. 

### 우사기(Usagi)

### Usagi

우사기(Usagi)는 코드 매핑의 절차를 도와주는 도구이다. Usagi는 코드 설명의 단어 유사도에 기반하여 매핑을 추천할 수 있다. 만약 원천 코드가 외국어로만 확인가능하다면, Google Translate[^GoogleTranslateUrl]를 통해 종종 해당 용어들의 훌륭한 영어 번역을 확인할 수 있다. Usagi는 사용자들로 하여금 자동 추천이 정확하지 않을 경우 적절한 목표 개념을 찾을 수 있도록 하고 있다. 최종적으로 사용자는 어떤 매핑이 ETL에 사용될 수 있는지 지정할 수 있다. Usagi는 GitHub[^UsagiUrl]을 통해 사용할 수 있다. \index{Usagi} \index{source code mapping|see {Usagi}}

Usagi is a tool to aid the manual process of creating a code mapping. It can make suggested mappings based on textual similarity of code descriptions. If the source codes are only available in a foreign language, we have found that Google Translate[^GoogleTranslateUrl] often gives surprisingly good translation of the terms into English. Usagi allows the user to search for the appropriate target concepts if the automated suggestion is not correct. Finally, the user can indicate which mappings are approved to be used in the ETL. Usagi is available on GitHub.[^UsagiUrl] \index{Usagi} \index{source code mapping|see {Usagi}}

[^GoogleTranslateUrl]: https://translate.google.com/
[^UsagiUrl]: https://github.com/OHDSI/Usagi

#### 범위와 목표 {-}

#### Scope and Purpose {-}

매핑이 필요한 원천 코드들은 Usagi를 통해 불러와진다(만약 코드들이 영어가 아닐경우, 추가적인 번역된 컬럼이 필요하다). 단어 유사도 접근법은 원천 코드와 Vocabulary 개념들을 연결시키기 위해 필요하다.하지만 이러한 코드 연결은 수동적으로 검토해야하고, Usagi는 이를 수행하기 위한 인터페이스를 제공한다. Usagi는 Vocabulary에 Standard인 concept들만을 제안한다.

Source codes that need mapping are loaded into the Usagi (if the codes are not in English additional translations columns are needed). A term similarity approach is used to connect source codes to Vocabulary concepts. However, these code connections need to be manually reviewed and Usagi provides an interface to facilitate that. Usagi will only propose concepts that are marked as Standard concepts in the Vocabulary. 

#### 절차 개요 {-}

#### Process Overview {-}

소프트웨어를 사용하기 위한 일반적인 순서:

1. 원천 시스템("원천 코드")로부터 Vocabulary concepts들로의 매핑을 진행하고 싶은 코드들을 올림.
2. Usagi 단어 유사도 접근법을 이용하여 Vocabulary concepts들로의 매핑을 진행.
3. Usagi 인터페이스를 활용하여 제안된 매핑을 확인하고 필요할 경우 개선. 
4. 매핑 결과를 Vocabulary의 SOURCE_TO_CONCEPT_MAP으로 내보냄.

The typical sequence for using this software is:

1. Load codes from your sources system (“source codes”) that you would like to map to Vocabulary concepts.
2. Usagi will run term similarity approach to map source codes to Vocabulary concepts.
3. Leverage Usagi interface to check, and where needed, improve suggested mappings. Preferably an individual who has experience with the coding system and medical terminology should be used for this review.
4. Export mapping to the Vocabulary’s SOURCE_TO_CONCEPT_MAP.

#### Usagi로의 원천 코드 가져오기

#### Importing Source Codes into Usagi {-}

원천 시스템에서 CSV나 엑셀(.xlsx) 파일로 원천 코드들을 내보낸다. 이 때 파일은 원천 코드와 영어 코드 설명에 대한 컬럼들이 있어야하지만, 추가적인 정보들도 역시 더해질 수 있다(e.g. 약물 용량, 번역되었을 경우 원래 언어로의 코드 설명). 게다가 원천 코드에 대한 정보 뿐만 아니라, 어떤 코드를 먼저 매핑해야할지 정하는데 도움이 되기 때문에 빈도 역시 포함되있는 것이 좋다(e.g. 1,000개의 원천 코드를 가져올 수 있지만, 100개만 실제 시스템에 정말로 사용되는 경우). 만약 원천 코드가 영어로의 번역이 필요할 경우, Google Translate이 도움이 될 수 있다.

참고: 원천 코드는 도메인(domain)에 의해 분류되어야하며, 하나의 파일로 묶여서는 안됨.

Export source codes from source system into a CSV or Excel (.xlsx) file. This should at least have columns containing the source code and an English source code description, however additional information about codes can be brought over as well (e.g. dose unit, or the description in the original language if translated). In addition to information about the source codes, the frequency of the code should preferably also be brought over, since this can help prioritize which codes should receive the most effort in mapping (e.g. you can have 1,000 source codes but only 100 are truly used within the system). If any source code information needs translating to English, use Google Translate to do that. 

Note: source code extracts should be broken out by domain (i.e. drugs, procedures, conditions, observations) and not lumped into one large file.

파일로부터 원천 코드를 Usagi로 올린다 -> 코드 메뉴를 가져온다. 여기서 “Import codes …”는 그림 \@ref(fig:usagiImport)에서처럼 보일 것이다. 이 그림에서 원천 코드 용어들은 네덜란드어이고, 영어로 번역되어있다. Usagi는 표준용어로의 매핑을 위해 영어 번역을 이용할 것이다.

Source codes are loaded into Usagi from the File –> Import codes menu. From here an “Import codes …” will display as seen in Figure \@ref(fig:usagiImport). In this figure, the source code terms were in Dutch and were also translated into English. Usagi will leverage the English translations to map to the standard vocabulary.

```{r usagiImport, fig.cap="Usagi source code input screen.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/usagiImport.png")
```

“Column mapping” 부분(왼쪽 아래)은 Usagi로 하여금 불러온 테이블을 어떻게 사용할 것인지 정하는 단계이다. 만약 마우스를 끌어다 놓으면, 각 컬럼들을 정의하는 팝업창이 나타날 것이다. Usagi는 원천 코드를 Vocaublary concept 코드에 연결시키는 정보로써 “Additional info” 컬럼을 사용하지 않을 것이다; 하지만 이 추가적인 정보는 개인이 원천 코드 매핑을 검토하는데 도움을 줄 수 있기에 포함되어야 한다.

The “Column mapping” section (bottom left) is where you define for Usagi how to use the imported table. If you mouse hover over the drop downs, a pop-up will appear defining each column. Usagi will not use the “Additional info” column(s) as information to associate source codes to Vocabulary concept codes; however, this additional information may help the individual reviewing the source code mapping and should be included. 

마지막으로 “Filters” 부분(아래 오른쪽)에서 Usagi로 매핑할 때의 몇 가지 제한을 설정할 수 있다. 예를 들어 그림 \@ref(fig:usagiImport)에서 사용자는 Condition 도메인에만 원천 코드를 매핑하고 있다. 기본적으로 Usagi는 Standard Ceoncepts에만 매핑을 진행하지만, 만약 “Filter standard concepts” 옵션이 아닐 경우, Usagi는 Classification Concepts 또한 검토할 것이다. 마우스를 다른 필터에 올려놓으면 해당 필터에 대한 추가적인 정보가 나타날 것이다.

Finally, in the “Filters” section (bottom right) you can set some restrictions for Usagi when mapping. For example, in Figure \@ref(fig:usagiImport), the user is mapping the source codes only to concepts in the Condition domain. By default, Usagi only maps to Standard Concepts, but if the option “Filter standard concepts” is turned off, Usagi will also consider Classification Concepts. Hover your mouse over the different filters for additional information about the filter.

한 가지 특별한 필터는 “Filter by automatically selected concepts / ATC code”이다. 만약 검색에 조건을 걸어야 한다면, 자동 concept ID로 표시되는 컬럼(세미콜론으로 구분)에 CONCEPT_ID 목록이나 ATC 코드를을 제공하여 하면 된다. 예를 들어 약물의 경우 이미 각 약들에 ATC 코드가 이미 할당되어 있을 수 이따. 비록 ATC 코드가 하나의 RxNorm 약물 코드로 인지되지 않더라도, Vocabulary의 ATC 코드 한정으로 검색을 제한하는데 도와줄 수 있다. ATC 코드를 사용하려면 다음 절차를 따르면 된다:

One special filter is “Filter by automatically selected concepts / ATC code”. If there is information that you can use to restrict the search, you can do so by providing a list of CONCEPT_IDs or an ATC code in the column indicated in the Auto concept ID column (semicolon-delimited). For example, in the case of drugs there might already be ATC codes assigned to each drug. Even though an ATC code does not uniquely identify a single RxNorm drug code, it does help limit the search space to only those concepts that fall under the ATC code in the Vocabulary. To use the ATC code, follow these steps: 

1. 컬럼 매핑 부분에서, "Auto concept ID column"을 "ATC column"로 바꾸어라.
2. 컬럼 매핑 부분에서, ATC 코드가 포함된 열을 "ATC column"으로 선택하여라.
3. "Filter by user selected concepts / ATC code" 필터를 눌러라.

1. In the Column mapping section, switch from "Auto concept ID column" to "ATC column"
2. In the Column mapping section, select the column containing the ATC code as "ATC column".
3. Turnon the "Filter by user selected concepts / ATC code" on in the Filters section.

또한 ATC 코드 이외의 다른 것들로도 조건을 설정할 수 있다. 위의 그림 예시에서 보여지듯이 우리는 UMLS의 부분 매핑을 이용하여 Usagi의 검색을 설정하였다. 이런 경우에는 “Auto concept ID column”을 사용하여야 한다.

You can also use other sources of information than the ATC code to restrict as well. In the example shown in the figure above, we used a partial mapping derived from UMLS to restrict the Usagi search. In that case we will need to use “Auto concept ID column”. 

일단 모든 설정을 마치고 나면, “Import” 버튼을 눌러서 파일을 불러와야한다. 파일 불러오기는 단어 유사도 알고리즘을 이용하여 원천 코드를 매핑하기 때문에 대략 몇 분정도 소요될 수 있다.  

Once all your settings are finalized, click the “Import” button to import the file. The file import will take a few minutes as it is running the term similarity algorithm to map source codes. 

#### 원천 코드의 Vocabulary Concept 매핑 검토 {-}

#### Reviewing Source Code to Vocabulary Concept Maps {-}

일단 원천 코드의 파일을 불러오면, 매핑 절차가 시작된다. 그림 \@ref(fig:usagiOverview)에서 Usagi 화면이 3가지 주요 그능으로 구분된 것을 확인할 수 있다: 개요 테이블, 선택된 매핑 테이블, 검색 기능. 이 때, 오른 클릭을 하여 어떤 테이블에 대해서도 컬럼들을 선택하여 숨기거나 가려서 시각적 복잡성을 줄일 수 있다는 것을 참고하면 된다.

Once you have imported your input file of source codes, the mapping process begins. In Figure \@ref(fig:usagiOverview), you see the Usagi screen is made up of 3 main sections: an overview table, the selected mapping section, and place to perform searches. Note that in any of the tables, you can right-click to select the columns that are shown or hidden to reduce the visual complexity.

```{r usagiOverview, fig.cap="Usagi source code input screen.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/usagiOverview.png")
```

#### 제안된 매핑의 승인 {-}

#### Approving a Suggested Mapping {-}

“Overview Table”은 현재의 원천 코드의 매핑을 보여준다. 원천 코드를 불러온 직후, 검색 설정과 단어 유사도를 기반으로 자동으로 생성되고 제안되는 매핑을 포함하고 있다. 그림 \@ref(fig:usagiOverview)에서 나타나듯이, 사용자가 검색 옵션을 Condition으로 설정했기에 네덜란드어 Condition 코드의 영어 이름은 Condition 도메인의 표준용어로 매핑되는 것을 볼 수 있다.  Usagi는 원천 코드 기술서의 컨셉 이름과 동의어를 비교함으로써 최적의 매칭을 수행한다. 사용자가 “Include source terms”를 선택하였기 때문에 Usagi는 vocabulary의 특정 코드로 매핑되는 모든 원천 코드의 이름과 동의어까지 검토하게 된다. 만약 Usagi가 매핑을 진행할 수 없을 경우, CONCEPT_ID = 0 으로 매핑될 것이다.

The “Overview Table” shows the current mapping of source codes to concepts. Right after importing source codes, this mapping contains the automatically generated suggested mappings based on term similarity and any search options. In the example in Figure \@ref(fig:usagiOverview), the English names of Dutch condition codes were mapped to standard concepts in the Condition domain, because the user restricted the search to that domain. Usagi compared the source code descriptions to concept names and synonyms to find the best match. Because the user had selected “Include source terms” Usagi also considered the names and synonyms of all source concepts in the vocabulary that map to a particular concept. If Usagi is unable to make a mapping, it will map to the CONCEPT_ID = 0. 

코딩 시스템에 익숙한 사람이 원천 코드의 표준 용어로의 매핑을 도와주는 것이 권장된다. 각 개인은 “Overview Table” 탭에서 각 코드에 대하여 Usagi가 권장하는 매핑을 받아들이거나 아니면 새로운 매핑을 선택하는 작업을 하게 된다. 예를 들어 그림 \@ref(fig:usagiOverview)에서 우리는 네덜란드어 “Hoesten”가 영어 “Cough”로 번역되는 것을 볼 수 있다. Usagi는 “Cough”를 그리고 Vocabualry 컨셉 “4158493-C/O - cough”로 매핑을 한다. 이 때의 매핑에 대하여 매칭 점수는 0.58(매칭 점수는 일반적으로 0에서 1의 값을 가짐)이였고, 이는 Usagi가 이 네덜란드어에 대하여 SNOMED에 대해 매핑한 결과에 대해 확실하게 제시하기 어렵다는 것을 의미한다. 이 예시에서는 해당 매핑에 동의하였고, 화면의 하단 우측의 “Approve” 버튼을 클릭함으로써 승인하였다.

It is suggested that someone with experience with coding systems help map source codes to their associated standard vocabulary. That individual will work through code by code in the “Overview Table” to either accept the mapping Usagi has suggested or choose a new mapping. For example in Figure \@ref(fig:usagiOverview) we see that the Dutch term “Hoesten” which was translated to the English term “Cough”. Usagi used “Cough” and mapped it to the Vocabulary concept of “4158493-C/O - cough”. There was a matching score of 0.58 associated to this matched pair (matching scores are typically 0 to 1 with 1 being a confident match), a score of 0.58 signifies that Usagi is not very sure of how well it has mapped this Dutch code to SNOMED. Let us say in this case, we are okay with this mapping, we can approve it by hitting the green “Approve” button in the bottom right hand portion of the screen.

#### 새로운 매핑의 탐색 {-}

#### Searching for a New Mapping {-}

Usagi가 제시해주는 매핑에 대하여 사용자가 새로운 매핑을 찾거나 아니면 컨셉이 없도록(CONCEPT_ID = 0) 하는 경우들도 있을 것이다. 그림 \@ref(fig:usagiOverview)의 예시를 통해 네덜란드어 “Hoesten”가 영어 “Cough”로 번역되는 것을 확인할 수 있었다. Usagi의 제안은 UMLS에서 파생된 매핑으로 제한되기에, 그 결과가 적합하지 않을 수도 있다. 검색 기능을 통해서 실제 용어 자체 혹은 검색 상자 쿼리를 이용해서 다른 컨셉들을 찾을 수 있다. 

There will be cases where Usagi suggests a map and the user will be left to either try to find a better mapping or set the map to no concept (CONCEPT_ID = 0). In the example given in Figure \@ref(fig:usagiOverview), we see for the Dutch Term “Hoesten”, which was translated to “Cough”. Usagi’s suggestion was restricted by the concept identified in our automatically derived mapping from UMLS, and the result might not be optimal. In the Search Facility, we could search for other concepts using either the actual term itself or a search box query. 

메뉴얼 검색 상자를 이용할 때, Usagi는 구조화된 검색 쿼리를 지원하지 않고 fuzzy search를 한다는 것을 기억하여야 한다. 그리고 현재까지는 AND나 OR과 같은 Boolean 연산자에 대해서는 검색을 지원하지 않고 있다. 

When using the manual search box, one should keep in mind that Usagi uses a fuzzy search, and does not support structured search queries, so for example not supporting Boolean operators like AND and OR. 

“Cough”에 대해서 더 나은 매핑을 찾는다고 가정해보자. 검색 기능의 오른편 쿼리 부분에 Vocabulary 검색을 할 때 결과를 정리해주는 기능을 제공하는 필터 부분이 있다. 이러한 경우에는 우리는 표준 용어만을 찾아야 하며, 표준 용어에 매핑되는 코드의 이름과 동의어를 기반으로 검색할 수 있다. 

To continue our example, suppose we used the search term “Cough” to see if we could find a better mapping. On the right of the Query section of the Search Facility, there is a Filters section, this provides options to trim down the results from the Vocabulary when searching for the search term. In this case we know we want to only find standard concepts, and we allow concepts to be found based on the names and synonyms of source concepts in the vocabulary that map to those standard concepts. 

이러한 검색 기준을 적용한다면 “254761-Cough”과 같은 코드를 찾을 수 있으며, 네덜란드어의 코드 매핑에 적합한 용어일 수 있다. 이를 위해 “Selected Source Code” 업데이트의 “Replace concept” 버튼을 누르고, “Approve” 버튼을 누르면 된다. 또한 “Add concept” 버튼이 있는데, 이는 하나의 원천 코드의 다수의 표준 용어 컨셉 매핑을 하게끔 한다(e.g.일부 원천 코드들은 표준 용어와는 달리 다양한 질병들을 함께 포함하고 있을 수 있다).

When we apply these search criteria we find “254761-Cough” and feel this may be an appropriate Vocabulary concept to map to our Dutch code. In order to do that we can hit the “Replace concept” button, which you will see in the “Selected Source Code” section update, followed by the “Approve” button. There is also an “Add concept” button, this allows for multiple standardized Vocabulary concepts to map to one source code (e.g. some source codes may bundle multiple diseases together while the standardized vocabulary may not). 

#### 컨셉 정보 {-}

#### Concept Information {-}

적절한 컨셉을 찾아 매핑을 하려 할 때, 컨셉의 “social life”을 고려하는 것은 중요하다. 컨셉의 의미는 계층 구조에서의 위치에 따라 부분적으로 의존적일 수 있으며, 종종 계층적 지위와 거의 혹은 전혀 상관없고 대상 컨셉으로도 적절하지 않은 “orphan concepts”이 있다. Usagi는 각 컨셉이 얼마나 많은 부모, 자식 컨셉들이 있는지 알려주기도 하고, ALT + C를 누르거나 위쪽 메뉴바의 view -> Concept을 누르면 더 자세한 정보를 볼 수 있다. 

When looking for appropriate concepts to map to, it is important to consider the “social life” of a concept. The meaning of a concept might depend partially on its place in the hierarchy, and sometimes there are “orphan concepts” in the vocabulary with few or no hierarchical relationships, which would be ill-suited as target concepts. Usagi will often report the number of parents and children a concept has, and it also possible to show more information by pressing ALT + C or selecting view –> Concept information in the top menu bar.


```{r usagiConceptInfo, fig.cap="Usagi concept information panel.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/usagiConceptInfo.png")
```


그림 \@ref(fig:usagiConceptInfo)은 컨셉 정보 패널을 보여준다. 컨셉의 일반적인 정보부터, 부모, 자식, 그리고 다른 원천 코드들과의 정보도 보여준다. 사용자는 이 패널을 이용해서 계층 구조를 탐색할 수 있고, 다른 목표 컨셉을 정할 수도 있다. 

Figure \@ref(fig:usagiConceptInfo) shows the concept information panel. It shows general information about a concept, as well as its parents, children, and other source codes that map to the concept. Users can use this panel to navigate the hierarchy and potentially choose a different target concept. 

모든 코드가 끝날 때까지 코드를 따라 이 절차를 진행하면 된다. 화면의 맨 위의 원천 코드 목록에서, 컬럼 헤딩 별로 코드들을 정렬할 수 있다. 종종 최빈도부터 최저빈도의 코드까지 살펴보는 것을 권장한다. 화면의 하단 왼쪽 편에는 매핑을 허용한 코드들의 개수, 그리고 그에 따라 얼마나 많은 코드들이 발생했는지를 확인할 수 있다. 

Continue to move through this process, code by code, until all codes have been checked. In the list of source codes at the top of the screen, by selecting the column heading you can sort the codes. Often, we suggest going from the highest frequency codes to the lowest. In the bottom left of the screen you can see the number of codes that have approved mappings, and how many code occurrences that corresponds to. 

왜 그러한 매핑 결정이 내렸는지를 문서화할 떄 사용될 수 있는 매핑에 설명을 추가할 수도 있다.

It is possible to add comments to mappings, which could be used to document why a mapping decision was made.

#### 최적의 절차 {-}

#### Best Practices {-}

* 코딩 스키마에 경험이 있는 사람이 할 것
* “Overview Table”의 컬럼들을 컬럼 이름을 눌러서 정렬할 수 있다. “Match Score”를 눌러서 정렬하는 것이 중요할 수도 있다; Usagi가 가장 확실하게 매핑하는 코드들을 검토하면 많은 코드들이 빠르게 작업이 끝날 수도 있다. 그리고 빈도가 높은 단어와 낮은 단어들에 쓰이는 노력이 다르기 때문에 “Frequency”로 정렬해서 작업하는 것도 중요하다. 
* CONCEPT_ID= 0으로 매핑하는 것도 괜찮지만, 어떤 코드들은 최적의 매핑 코드들이 없을 수도 있고, 어떤 것들은 단지 마땅한 코드들이 없어서 일 수도 있다.
* 특히 부모 계층과 자녀 계층에 대해서는 컨셉의 내용을 고려하는 것이 중요하다. 

* Use someone who has experience with coding schemes. 
* By clicking on a column name, you can sort the columns in the “Overview Table”. It may be valuable to sort on “Match Score”; reviewing codes that Usagi is most confident on first may quickly knock out a significant chunk of codes. Also sorting on “Frequency” is valuable, spending more effort on frequent codes versus non-frequent is important. 
* It is okay to map some codes to CONCEPT_ID = 0, some codes may not be worth it to find a good map and others may just lack a proper map. 
* It is important to consider the context of a concept, specifically its parents and children. 

#### Usagi 매핑 내보내기 {-}

#### Export the Usagi Map Created {-}

일단 USAGI를 통해 매핑을 생성하였으면, 사용하기 가장 좋은 방법은 매핑을 내보낸 다음 Vocabulary SOURCE_TO_CONCEPT_MAP 테이블에 추가시키는 것이다. 

Once you have created your map within USAGI, the best way to use it moving forward is to export it and append it to the Vocabulary SOURCE_TO_CONCEPT_MAP table. 

매핑을 내보내기 위해서는, File -> Export source_to_concept_map으로 가면 된다. 이 때 어느 SOURCE_VOCABULARY_ID를 이용할 것인지 묻는 팝업창이 나타나는데 짧은 식별자를 입력하면 된다. Usagi는 이 식별자를 이용할 것이다. SOURCE_VOCABULARY_ID가 SOURCE_TO_CONCEPT_MAP 테이블의 특정 매핑을 식별할 수 있게끔 할 수 있다.

To export your mappings, go to File –> Export source_to_concept_map. A pop-up will appear asking you which SOURCE_VOCABULARY_ID you would like to use, type in a short identifier. Usagi will use this identifier. as the SOURCE_VOCABULARY_ID which will allow you to identify your specific mapping in the SOURCE_TO_CONCEPT_MAP table. 

SOURCE_VOCABULARY_ID를 선택한 후에는, 내보낼 CSV 파일의 이름과 파일 경로를 입력하게 된다. 내보내는 CSV 파일의 구조는 SOURCE_TO_CONCEPT_MAP 테이블과 동일하다. 이 매핑은 Vocabulary의 SOURCE_TO_CONCEPT 테이블로 추가될 수 있다. 그리고 앞선 단계에서 정의한 SOURCE_VOCABULARY_ID를 정의하는 VOCABULARY 테이블에 단일 행으로 추가하는 것이 좋다. 마지막으로, “Approved” 상태인 매핑들만을 CSV 파일로 내보내는 것이 중요하다; 매핑을 내보내기 위해서는 USAGI에서 매핑을 완료해야만 한다.

After selecting the SOURCE_VOCABULARY_ID, you give your export CSV a name and save to location. The export CSV structure is in that of the SOURCE_TO_CONCEPT_MAP table. This mapping could be appended to the Vocabulary’s SOURCE_TO_CONCEPT_MAP table. It would also make sense to append a single row to the VOCABULARY table defining the SOURCE_VOCABULARY_ID you defined in the step above. Finally, it is important to note that only mappings with the “Approved” status will be exported into the CSV file; the mapping needs to be completed in USAGI in order to export it. 

#### Usagi 매핑 업데이트 {-}

#### Updating an Usagi Mapping {-}

매핑은 종종 한 번에 끝나지 않는다. 원천 코드가 추가되는 식으로 데이터가 업데이트 되거나 용어가 정기적으로 업데이트되면 매핑 또한 업데이트 되어야 할 것이다.

Often a mapping is not a one-time effort. As data is updated perhaps new source codes are added, and the vocabulary is updated regularly, perhaps requiring an update of the mapping. 

원천 코드가 업데이트될 때는 다음과 같은 단계들을 따르는 것이 좋다:

When the set of source codes is updated the following steps can be followed to support the update:

1. 새로운 원천 코드 파일을 불러온다.
2. 파일을 고른다 -> 이전의 매핑을 적용시키고, 예전의 Usagi 매핑 파일을 선택한다.
3. 이전의 매핑 파일에서 매핑되지 않았던 코드들을 식별하고, 새롭게 매핑한다.

1. Import the new source code file 
2. Choose File –> Apply previous mapping, and select the old Usagi mapping file 
3. Identify codes that haven't inherited approved mappings from the old mapping, and map them as usual.

용어사 업데이트 되면 아래의 단계를 따른다:

When the vocabulary is updated, follow these steps: 

1. Athena에서 새로운 용어 파일들을 다운받는다.
2. Usagi 인덱스를 새로 빌딩한다(Help -> Rebuild index).
3. 매핑 파일을 연다.
4. 새로운 용어 버전에 따라 표준 용어가 아닌 코드들을 식별하여 적절한 목표 컨셉들을 찾는다.

1. Download the new vocabulary files from Athena 
2. Rebuild the Usagi index (Help –> Rebuild index) 
3. Open the mapping file 
4. Identify codes that map to concepts that in the new vocabulary version no longer are Standard concepts, and find more appropriate target concepts.

## 3 단계: ETL 수행

## Step 3: Implement the ETL

일단 설계와 코드 매핑이 완료된다면, ETL 절차는 소프트웨어를 통해 수행가능하다. ETL이 설계될 때, CDM과 원천 데이터 둘 다에 대해 잘 아는 사람이 참여하기를 권장하였다. 마찬가지로 ETL이 수행될 때도, 데이터(특히 빅데이터)와 ETL 수행에 경험이 있는 사람이 참여하는 것이 바람직하다. 즉, 이건 그룹 외부의 기술 전문가를 고용하거나 불러서 ETL 수행을 시키는 것일지도 모른다. 또한 이건 한 번에 끝나는 작업이 아니라는 점을 참고하기 바란다. 그렇기에 앞으로는 ETL 수행 및 유지에 일정 시간 이상을 할애할 수 있는 사람이나 팀이 있는 것이 좋을 것이다(\@ref(CDMandETLMaintenance)에서 더 명확히 설명할 것이다). 

Once the design and code mappings are completed, the ETL process can be implemented in a piece of software. When the ETL was being designed we recommended that people who are knowledgeable about the source and CDM work together on the task. Similarly, when the ETL is being implemented it is preferred to use people who have experience with working with data (particularly large data) and experience with implementing ETLs. This may mean working with individuals outside of your immediate group or hiring technical consultants to execute the implementation. It is also important to note that this is not a one-time expense.  Moving forward it would be good to have someone or a team who spends at least some dedicated time to maintaining and running the ETL (this will become clearer in Section \@ref(CDMandETLMaintenance)). 

수행은 각 기관에 따라 다양한 양상을 보이며 특히 인프라, 데이터베이스의 크기, ETL의 복잡성, 기술 전문가의 능력 등의 요소에 따라 많이 달라진다. 많은 요소들에 따라 달라지기 때문에 OHDSI는 ETL을 수행하기 위한 최선의 방법의 공식적인 권고를 하고 있지 않다. 그 동안 많으ㄴ 그룹들이 SQL builders, SAS, C#, Java, Kettle들을 사용했다. 각각 장점들과 단점들이 있었고, 그 어느 것도 이러한 기술에 익숙한 사람이 없으면 아무것도 사용할 수 없었다. 

Implementation usually varies site to site and it largely depends on many factors including infrastructure, size of the database, the complexity of the ETL, and the technical expertise available. Because it depends on many factors the OHDSI community does not make a formal recommendation on how best to implement an ETL. There have been groups that use simple SQL builders, SAS, C#, Java, and Kettle. All have their advantages and disadvantages, and none are usable if there is nobody at the site who is familiar with the technology. 

각각 다른 ETL의 예 (복잡성에 따라 정렬됨):

* ETL-Synthea - SQL을 이용한 Synthea 데이터베이스 변환
  + [https://github.com/OHDSI/etl-synthea](https://github.com/OHDSI/etl-synthea)
* ETL-CDMBuilder - 다수의 데이터베이스를 변환하기 위해 고안된 .NET application
  + [https://github.com/OHDSI/etl-cdmbuilder](https://github.com/OHDSI/etl-cdmbuilder)
* ETL-LambdaBuilder - AWS lamda 기능을 이용한 빌더 
  + [https://github.com/OHDSI/etl-lambdabuilder](https://github.com/OHDSI/etl-lambdabuilder)

A few examples of different ETLs (listed in order of complexity):\index{ETL!implementations}

* ETL-Synthea - A SQL builder written to convert the Synthea database
  + [https://github.com/OHDSI/etl-synthea](https://github.com/OHDSI/etl-synthea)
* ETL-CDMBuilder - A .NET application designed to transform multiple databases
  + [https://github.com/OHDSI/etl-cdmbuilder](https://github.com/OHDSI/etl-cdmbuilder)
* ETL-LambdaBuilder - A builder using the AWS lambda functionality
  + [https://github.com/OHDSI/etl-lambdabuilder](https://github.com/OHDSI/etl-lambdabuilder)
  
그동안 많은 시도들이 있었지만, ‘ultimate’한 사용자 친화적인 ETL 도구를 개발하는데에는 포기했다. 항상 많은 경우들에 있어서 도구들은 80%까지는 ETL을 잘 수행하지만, 남은 20%에 있어서는 원천 데이터베이스에 따라 낮은 단계에서의 코드 작성이 필요하다.
  
It should be noted that after several independent attempts, we have given up on developing the ‘ultimate’ user-friendly ETL tool. It is always the case that tools like that work well for 80% of the ETL, but for the remaining 20% of the ETL some low-level code needs to be written that is specific to a source database 

일단 기술 전문가가 수행할 준비가 된다면, ETL 설계 문서가 그들과 공유되어야만 한다. 문서에는 수행을 시작할만한 충분한 정보가 있어야하지만, 개발자들은 항상 개발 과정 중에 ETL 설계자들에게 질문할 수 있는 환경 역시 마련되있어야한다. ETL 설계자들에게는 로직이 명확해 보일지라도 CDM이나 데이터에 친숙하지 않은 수행자들에게는 명확하지 않아 보일 수도 있다. 그렇기에 수행 단계는 팀이 수행하는 단계이여야 한다. 설계자들과 수행자들 모두 로직이 올바르게 작동한다고 동의할 때까지,둘 사이에서 CDM 생성과 검증을 수행하는 것으로 간주되곤 한다.

Once the technical individuals are ready to start implementing, the ETL design document should be shared with them. There should be enough information in the documentation for them to get started however it should be expected that the developers have access to the ETL designers to ask questions during their development process.  Logic that may be clear to the designers may be less clear to an implementer who might not be familiar with the data and CDM. The implementation phase should remain a team effort. It is considered acceptable practice to go through the process of CDM creation and testing between the implementers and designers, respectively, until both groups are in agreement that all logic has been executed correctly.  

## 4 단계: 질 관리

## Step 4: Quality Control

추출, 변환, 적재의 절차 수행을 위해서 질 관리는 반복적으로 수행된다. 전형적인 패턴은 로직 작성 -> 로직 수행 -> 로직 검증 -> 로직 수정 및 작성 이다. CDM을 검증하기 위한 많은 방법들이 있지만, 아래의 단계들은 몇 년간의 ETL 수행을 통해 공동체 내부에서 권고하는 단계들이다. 
\index{ETL!quality control}

For the extract, transform, load process, quality control is iterative. The typical pattern is to write logic -> implement logic -> test logic -> fix/write logic. There are many ways to go about testing a CDM but below are recommend steps that have been developed across the community through years of ETL implementation. \index{ETL!quality control}

* ETL 설계 문서, 컴퓨터 코드, 코드 매핑을 검토하라. 어느 한 사람이라도 실수를 할 수 있기 때문에, 항상 한 명 이상  다른 사람이 무엇 작업을 수행하고 있는지 검토해야한다. 
  + 컴퓨터 코드의 가장 큰 문제점은 원천 데이터의 워천 코드가 표준 용어에 어떻게 매핑되었는지에 따라 나타나는 경향이 있다. 매핑은 특히 NDC처럼 날짜와 관련있을 경우 어려울 수 있다. 원천 용어가 항상 적절한 컨섭으로 변환되도록 매핑이 수행되는 부분을 두 번씩 봐야한다. 
* 원천 데이터와 목표 데이터의 샘플로써 한 사람의 모든 정보를 수동으로 비교하라.
  + 여러 개의 기록을 갖고 있는 한 사람의 데이터를 살펴보는 것이 도움이 될 수 있다. 한 사람의 기록을 추적함으로써 CDM 데이터가 로직에 따라 기대했던 결과와 다른 경우들을 발견해낼 수도 있다. 
* 원천 데이터와 목표 데이터의 전체 수를 비교하라.
  + 특정 문제들을 어떻게 해결하는지 설명하기에 따라 몇 가지 차이점들이 있을 수 있다. 예를 들면, 어떤 사람들은 성별이 NULL로 기록된 사람들은 어차피 연구에 포함되지 않기 때문에 삭제하기로 결정할 수도 있다. 그리고 CDM에서의 방문이나 원천 데이터에서의 방문이 다르게 구성될 수도 있다. 따라서 CDM과 원천 데이터의 총합을 비교할 때는 이러한 차이점을 예상하고 설명할 수 있어야한다. 
* 해당 CDM 버전에 대해 수행된 기존의 연구를 수행해본다. 

* Review of the ETL design document, computer code, and code mappings. Any one person can make mistakes, so always at least one other person should review what the what was done. 
  + The largest issues in the computer code tend to come from how the source codes in the native data are mapped to Standard Concepts. Mapping can get tricky, especially when it comes to date-specific codes like NDCs. Be sure to double check any area where mappings are done to ensure the correct source vocabularies are translated to the proper concept id.  
* Manually compare all information on a sample of persons in the source and target data.
  + It can be helpful to walk through one person’s data, ideally a person with a large number of unique records. Tracing through a single person can highlight issues if the data in the CDM is not how you expect it to look based on the agreed upon logic. 
* Compare overall counts in the source and target data.
  + There may be some expected differences in counts depending on how you chose to address certain issues. For instance, some collaborators choose to drop any people with a NULL gender since those people will not be included in analyses anyway. It may also be the case that visits in the CDM are constructed differently than visits or encounters in the native data. Therefore, when comparing overall counts between the source and CDM data be sure to account for and expect these differences.  
* Replicate a study that has already been performed on the source data on the CDM version.
  + This is a good way to understand any major differences between the source data and CDM version, though it is a little more time-intensive.
* Create unit tests meant to replicate a pattern in the source data that should be addressed in the ETL. For example, if your ETL specifies that patients without gender information should be dropped, create a unit test of a person without a gender and assess how the builder handles it.
  + Unit testing is very handy when evaluating the quality and accuracy of an ETL conversion. It usually involves creating a much smaller dataset that mimics the structure of the source data you are converting. Each person or record in the dataset should test a specific piece of logic as written in the ETL document. Using this method, it is easy to trace back issues and to identify failing logic. The small size also enables the computer code to execute very quickly allowing for faster iterations and error identification.

These are high-level ways to approach quality control from an ETL standpoint. For more detail on the data quality efforts going on within OHDSI, please see Chapter \@ref(DataQuality).

## ETL Conventions and THEMIS

As more groups converted data to the CDM it became apparent that conventions needed to be specified. For example, what should the ETL do in a situation where a person record lacks a birth year? The goal of the CDM is to standardized healthcare data however if every group handles data specific scenarios differently it makes it more difficult to systematically use data across the network.  

The OHDSI community started documenting conventions to improve consistency across CDMs. These defined conventions, that the OHDSI community has agreed upon, can be found on the CDM Wiki.[^cdmWikiUrl2] Each CDM table has its own set of conventions that can be referred to when designing an ETL. For example, persons are allowed to be missing a birth month or day, but if they lack a birth year the person should be dropped. In designing an ETL, refer to the conventions to help make certain design decisions that will be consistent with the community.  

While it will never be possible to document all possible data scenarios that exist and what to do when they occur, there is an OHDSI work group trying to document common scenarios. THEMIS[^themisUrl] is made up of individuals in the community who gather conventions, clarify them, share them with the community for comment, and then document finalized conventions in the CDM Wiki. Themis is an ancient Greek Titaness of divine order, fairness, law, natural law, and custom which seemed a good fit for this groups remit. When performing an ETL, if there is a scenario that you are unsure how to handle, THEMIS recommends that a question about the scenario is posed on the OHDSI Forums.[^ohdsiForum] Most likely if you have a question, others in the community probably have it as well. THEMIS uses these discussions, as well as work group meetings and face-to-face discussions, to help inform what other conventions need to be documented.        

[^cdmWikiUrl2]: https://github.com/OHDSI/CommonDataModel/wiki].
[^themisUrl]: https://github.com/OHDSI/Themis
[^ohdsiForum]: http://forums.ohdsi.org/

## CDM and ETL Maintenance {#CDMandETLMaintenance}

It is no small effort to design the ETL, create the mappings, implement the ETL, and build out quality control measures. Unfortunately, the effort does not stop there. There is a cycle of ETL maintenance that is a continuous process after the first CDM is built. Some common triggers that require maintenance are:  changes in the source data, a bug in the ETL, a new OMOP Vocabulary is released, or the CDM itself has changed or updated. If one of these triggers occur the following might need updating: the ETL documentation, the software programming running the ETL, and test cases and quality controls. 

Often a healthcare data source is forever changing. New data might be available (e.g. a new column in the data might exist). Patience scenarios that never existed before suddenly do (e.g. a new patient who has a death record before they were born). Your understanding of the data may improve (e.g. some of the records around inpatient child birth come across as outpatient due to how claims are processed). Not all changes in the source data my trigger a change in the ETL processing of it, however at a bare minimum the changes that break the ETL processing will need to be addressed. 

If bugs are found, they should be addressed. However, it is important to keep in mind that not all bugs are created equal. For example, let say in the COST table the column cost was being rounded to a whole digit (e.g. the source data had \$3.82 and this became \$4.00 in the CDM). If the primary researchers using the data were mostly performing characterizations of patient’s drug exposures and conditions, a bug such as this is of little importance and can be addressed in the future. If the primary researchers using the data also included health economists, this would be a critical bug that need to be addressed immediately. 

The OMOP Vocabulary is also ever changing just as our source data may be. In fact, the Vocabulary can have multiple releases in one given month as vocabularies update. Each CDM is run on a specific version of a Vocabulary and running on a newer improved Vocabulary could result in changes in how sources codes get mapped to in the standardized vocabularies. Often differences between Vocabularies are minor, so building a new CDM every time a new Vocabulary is release is not necessary. However, it is good practice to adopt a new Vocabulary once or twice a year which would require reprocessing the CDM again. It is rare are there changes in a new version of a Vocabulary that would require the ETL code itself to be updated. 

The final trigger that could require CDM or ETL maintenance is when the common data model itself updates. As the community grows and new data requirements are found this may lead to additional data being stored in the CDM. This might mean data that you previously were not storing in the CDM might have a location in a new CDM version. Less frequently are changes to existing CDM structure, however it is a possibility. For example, the CDM is moved to adopting DATETIME fields over the original DATE fields would could cause an error in ETL processing. CDM versions are not released often and sites can choose when they migrate.

## Final Thoughts on ETL

The ETL process is a difficult one to master for many reasons, not the least of which the fact that we are all working off unique source data, making it hard to create a “one-size-fits-all” solution. However, there are some hard won lessons we have learned over the years. 

- The 80/20 rule. If you can avoid it do not spend too much time manually mapping source codes to concepts sets. Ideally, map the source codes that cover the majority of your data. This should be enough to get you started and you can address any remaining codes in the future based on use cases. 
- It’s ok if you lose data that is not of research quality. Often these are the records that would be discarded before starting an analysis anyway, we just remove them during the ETL process instead. 
- A CDM requires maintenance. Just because you complete an ETL does not mean you do not need to touch it ever again. Your raw data might change, there might be a bug in the code, there may be new vocabulary or an update to the CDM. Plan for an allocate resources to these changes so your ETL is always up-to-date. 
- For support with getting started with the OHDSI CDM, performing your database conversion, or running the analytics tools, please visit our Implementers Forum.[^implementersForum]

[^implementersForum]: https://forums.ohdsi.org/c/implementers

## Summary

```{block2, type='rmdsummary'}
- There is a generally agreed upon process for how to approach an ETL, including
  - Data experts and CDM experts together design the ETL
  - People with medical knowledge create the code mappings
  - A technical person implements the ETL
  - All are involved in quality control

- Tools have been developed by the OHDSI community to facilitate these steps and are freely available for use

- There are many ETL examples and agreed upon conventions you can use as a guide

```

## Exercises

```{exercise, exerciseEtl1}
Put the steps of the ETL process in the proper order:
  
A) Data experts and CDM experts together design the ETL
B) A technical person implements the ETL
C) People with medical knowledge create the code mappings
D) All are involved in quality control 

```

```{exercise, exerciseEtl2}
Using OHDSI resources of your choice, spot four issues with the PERSON record show in Table \@ref(tab:exercisePersonTable) (table abbreviated for space):

Table: (\#tab:exercisePersonTable) A PERSON table.

Column | Value
:---------------- |:-----------
PERSON_ID | A123B456
GENDER_CONCEPT_ID | 8532
YEAR_OF_BIRTH | NULL
MONTH_OF_BIRTH | NULL
DAY_OF_BIRTH | NULL
RACE_CONCEPT_ID | 0
ETHNICITY_CONCEPT_ID | 8527
PERSON_SOURCE_VALUE | A123B456
GENDER_SOURCE_VALUE | F
RACE_SOURCE_VALUE | WHITE
ETHNICITY_SOURCE_VALUE | NONE PROVIDED

```

```{exercise, exerciseEtl3}
Let us try to generate VISIT_OCCURRENCE records.  Here is some example logic written for Synthea:
Sort data in ascending order by PATIENT, START, END. Then by PERSON_ID, collapse lines of claim as long as the time between the END of one line and the START of the next is <=1 day. Each consolidated inpatient claim is then considered as one inpatient visit, set:
  
- MIN(START) as VISIT_START_DATE
- MAX(END) as VISIT_END_DATE
- "IP" as PLACE_OF_SERVICE_SOURCE_VALUE

If you see a set of visits as shown in Figure \@ref(fig:exerciseSourceData) in your source data, how would you expect the resulting VISIT_OCCURRENCE record(s) to look in the CDM?

```

```{r exerciseSourceData, fig.cap='Example source data.',echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/ExtractTransformLoad/exerciseSourceData.png")
```

Suggested answers can be found in Appendix \@ref(Etlanswers).
